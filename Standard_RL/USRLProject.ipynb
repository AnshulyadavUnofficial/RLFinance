{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d29e4730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tf_agents.environments import TFPyEnvironment, validate_py_environment\n",
    "\n",
    "from tf_agents.trajectories import StepType, Trajectory, PolicyInfo\n",
    "from tf_agents.networks import  Network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.utils import common\n",
    "from ddqn_agent import DdqnAgent\n",
    "\n",
    "from keras.optimizers.schedules import PolynomialDecay, PiecewiseConstantDecay\n",
    "import keras\n",
    "from game_environment import cache_directory_files, USStockEnv, StockState, USStockEnv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from sumTreeBatched import PrioritizedExperienceReplayBuffer  # python buffer\n",
    "from per_buffer import PrioritizedExperienceReplayBuffer    # c++ buffer\n",
    "from pprint import pprint\n",
    "from typing import Tuple, NamedTuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4d56ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data Collection\n",
    "tickers = [\"AAPL\", \"TSLA\", \"MSFT\", \"META\", \"GOOG\", \"AMZN\", \"NVDA\",\"AVGO\",\"JPM\",\"WMT\", \"LLY\",\"NFLX\",\"V\",\"ORCL\",\"MA\",\"XOM\",\"JNJ\",\"BAC\",\"PLTR\",\"ABBV\"]\n",
    "check_ticker = \"^GSPC\"\n",
    "\n",
    "# from USstockHelper import create_previous_data_file, printStaticGraph\n",
    "# # date_str = \"2025-09-02\" # Sep 2 was labour day\n",
    "# create_previous_data_file([check_ticker]+tickers, date_str=date_str)\n",
    "# printStaticGraph(date_str, check_ticker)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a07d76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop & Evaluation\n",
    "num_iterations           = 120000           # Total number of training iterations\n",
    "log_interval             = 500              # Steps between scalar/log prints\n",
    "num_eval_episodes        = 2                # Episodes to run per evaluation\n",
    "eval_interval            = 1500              # Steps between evaluations\n",
    "\n",
    "# Time-Step & Sequence Length\n",
    "N_step                      = 5             # Number of steps used in n-step return\n",
    "T                           = N_step + 1    # Total frames in each training sequence\n",
    "L                           = 10            # total number of observations in each state  \n",
    "\n",
    "# Environment & Episode Configuration\n",
    "collect_steps_per_iteration = 128          # New env steps per iteration (T defined above)\n",
    "regularization              = 0.0            # L2 regularization strength\n",
    "\n",
    "# Return & Reward Scaling\n",
    "gamma                    = 0.999           # Discount factor for future rewards\n",
    "reward_scalar            = 600               # Factor by which raw rewards are scaled\n",
    "\n",
    "# Target Network Updates\n",
    "target_update_tau        = 0.05              # Soft-update interpolation factor\n",
    "target_update_period     = 100              # Steps between hard-copy target updates\n",
    "\n",
    "# Learning Rate Schedule\n",
    "init_fc_lr                  = 1e-3                 # Starting learning rate\n",
    "final_lr                    = 1e-5                            # Minimum learning rate for both subnetworks\n",
    "num_decay_steps          = int(0.8 * num_iterations)  # Steps over which LR decays from init → final\n",
    "\n",
    "# Exploration (ε-Greedy)\n",
    "epsilon_start            = 0.5             # Starting ε (fully random)\n",
    "epsilon_end              = 0.05              # Final ε after decay\n",
    "\n",
    "#Experience Replay (PER)\n",
    "per_collect_steps           = 2**15            # Initial warm-up steps before any training\n",
    "per_replay_capacity         = per_collect_steps            # Maximum transitions stored in buffer\n",
    "per_replay_alpha            = 0.5              # Prioritization exponent (0 = uniform, 1 = full)\n",
    "per_replay_beta             = 0.6              # Initial importance-sampling correction factor\n",
    "per_delta_beta              = (1 - per_replay_beta) /num_decay_steps # β increment per training iteration\n",
    "num_per_samples             = 128              # Samples drawn from PER per training step\n",
    "max_window_size             = 2 * 390          # Max trajectories per add_batch_experience call\n",
    "num_prev_file               = 10                # number of previous to include in the calculation of histoircal previous average\n",
    "\n",
    "# Cross Valiation Parameters\n",
    "num_folds                   =   4       # K in K fold cross validation\n",
    "num_fold_update_interval    = (eval_interval/num_folds)   # every num_fold_update steps, the next fold is used. \n",
    "                                                    # Use eval_interval_num_folds, so that every eval \n",
    "                                                    # interval all folds have been used for validation once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdfbaaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(2, dtype=int32))\n",
      "{'independent': TensorSpec(shape=(10, 6), dtype=tf.float32, name='ind_obs'), 'dependent': TensorSpec(shape=(10, 4), dtype=tf.float32, name='dep_obs'), 'compound': TensorSpec(shape=(6,), dtype=tf.float32, name='comp_obs')}\n",
      "{'training': TensorSpec(shape=(), dtype=tf.float32, name='reward'), 'evaluation': TensorSpec(shape=(), dtype=tf.float32, name='evaluation')}\n",
      "616 training files, 205 validation files, 94 test files\n"
     ]
    }
   ],
   "source": [
    "# Creating the train, validation and test data_caches and environments\n",
    "class Fold(NamedTuple):\n",
    "    train: dict\n",
    "    files_train:list\n",
    "    validation: dict\n",
    "    files_validation:list\n",
    "\n",
    "\n",
    "train_dir = \"../US_Market_Data\"         # for training and validation data\n",
    "eval_dir = \"../US_Market_Data/^GSPC\"    # for test data\n",
    "\n",
    "train_cache = cache_directory_files(directory=train_dir, exclude_sub_dir=eval_dir, num_prev_file=num_prev_file) # eval_dir is excluded from training data\n",
    "test_cache = cache_directory_files(directory=eval_dir, num_prev_file=num_prev_file)\n",
    "\n",
    "\"\"\" Creating the validation and training folds\"\"\"\n",
    "folds_list = [{} for _ in range(num_folds)]\n",
    "fold_index = 0\n",
    "\n",
    "# round-robin partioning\n",
    "for k,v in train_cache.items():\n",
    "    folds_list[fold_index][k] = v\n",
    "    fold_index = (fold_index + 1) % num_folds\n",
    "\n",
    "folds:List[Fold] = []\n",
    "for index in range(num_folds):\n",
    "    v = folds_list[index]\n",
    "    t = {}\n",
    "    for i, cache in enumerate(folds_list):\n",
    "        if i == index: continue\n",
    "        t.update(cache)\n",
    "    folds.append(Fold(train=t, files_train= list(t.keys()), validation=v, files_validation= list(v.keys())))\n",
    "    \n",
    "del folds_list, train_dir, eval_dir\n",
    "\n",
    "\"\"\" Creating the ennvironments \"\"\"\n",
    "# training environment, just for training, no evaluations\n",
    "train_py_env = USStockEnv( folds[fold_index].train, reward_scalar, L)\n",
    "train_tf_env = TFPyEnvironment(train_py_env)\n",
    "\n",
    "# validation environments for both literal money and average return evaluations\n",
    "validation_py_env = USStockEnv( folds[fold_index].validation, reward_scalar, L)\n",
    "validation_tf_env = TFPyEnvironment(validation_py_env)\n",
    "\n",
    "# test environments for both literal money and average return \n",
    "test_py_env = USStockEnv(test_cache, reward_scalar, L)\n",
    "test_tf_env = TFPyEnvironment(test_py_env)\n",
    "\n",
    "validate_py_environment(train_py_env, episodes=10)\n",
    "validate_py_environment(validation_py_env, episodes=10)\n",
    "validate_py_environment(test_py_env, episodes=10)\n",
    "\n",
    "print( train_tf_env.action_spec())\n",
    "print( train_tf_env.observation_spec())\n",
    "print( train_tf_env.reward_spec())\n",
    "print(f\"{len(folds[fold_index].train)} training files, {len(folds[fold_index].validation)} validation files, {len(test_cache)} test files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08c8d439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CustomNetwork\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " INDEPENDENT_LSTM (LSTM)     multiple                  31280     \n",
      "                                                                 \n",
      " DEPENDENT_LSTM (LSTM)       multiple                  30600     \n",
      "                                                                 \n",
      " COMPOUND_EMBEDING_MLP (Seq  (1, 86)                   8428      \n",
      " uential)                                                        \n",
      "                                                                 \n",
      " RESIDUAL_BLOCK_0 (Sequenti  (1, 256)                  132608    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " RESIDUAL_BLOCK_1 (Sequenti  (1, 256)                  132608    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " RESIDUAL_BLOCK_2 (Sequenti  (1, 256)                  132608    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " RESIDUAL_BLOCK_3 (Sequenti  (1, 256)                  132608    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " RESIDUAL_BLOCK_4 (Sequenti  (1, 256)                  132608    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " RESIDUAL_BLOCK_5 (Sequenti  (1, 256)                  132608    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " RESIDUAL_BLOCK_6 (Sequenti  (1, 256)                  132608    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " VALUE_HEAD (Dense)          multiple                  257       \n",
      "                                                                 \n",
      " ADVANTAGE_HEAD (Dense)      multiple                  771       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 999592 (3.81 MB)\n",
      "Trainable params: 999592 (3.81 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class CustomNetwork(Network):\n",
    "    def __init__(self, observation_spec, action_spec, regularization, name=\"CustomNetwork\"):\n",
    "        super(CustomNetwork, self).__init__(input_tensor_spec=observation_spec, state_spec=(), name=name)\n",
    "        \n",
    "        # Save parameters\n",
    "        self._observation_spec = observation_spec\n",
    "        self._action_spec = action_spec\n",
    "        self._regularization = regularization\n",
    "        self._regularizer = keras.regularizers.l2(self._regularization)\n",
    "        self._num_actions = action_spec.maximum - action_spec.minimum + 1\n",
    "        self._name = name\n",
    "      \n",
    "        self.independent_lstm = keras.layers.LSTM(units=85, name=\"INDEPENDENT_LSTM\", kernel_regularizer=self._regularizer)\n",
    "        self.dependent_lstm = keras.layers.LSTM(units=85, name=\"DEPENDENT_LSTM\",kernel_regularizer=self._regularizer)\n",
    "        \n",
    "        self.compound_embedding_mlp = keras.Sequential([\n",
    "            keras.layers.Dense(86, activation=None, kernel_regularizer=self._regularizer),\n",
    "            keras.layers.LayerNormalization(),\n",
    "            keras.layers.LeakyReLU(),\n",
    "            keras.layers.Dense(86, activation=None, kernel_regularizer=self._regularizer),\n",
    "            keras.layers.LayerNormalization(),\n",
    "            keras.layers.LeakyReLU()\n",
    "        ], name=\"COMPOUND_EMBEDING_MLP\")\n",
    "        \n",
    "        # Create residual blocks (each with 2 layers)\n",
    "        self.residual_blocks = []\n",
    "        for i in range(7):\n",
    "            block = keras.Sequential([\n",
    "                keras.layers.Dense(256, activation=None, kernel_regularizer=self._regularizer),\n",
    "                keras.layers.LayerNormalization(),\n",
    "                keras.layers.LeakyReLU(),\n",
    "                keras.layers.Dense(256, activation=None, kernel_regularizer=self._regularizer),\n",
    "                keras.layers.LayerNormalization()\n",
    "            ], name=f\"RESIDUAL_BLOCK_{i}\")\n",
    "            self.residual_blocks.append(block)\n",
    "        \n",
    "        self.value_head = keras.layers.Dense(units=1, activation=None, name=\"VALUE_HEAD\", kernel_regularizer=self._regularizer)\n",
    "        self.advantage_head = keras.layers.Dense(units=self._num_actions, name=\"ADVANTAGE_HEAD\", kernel_regularizer=self._regularizer)\n",
    "    \n",
    "    def call(self, inputs, network_state=(), training=False):\n",
    "        ind = tf.cast(inputs['independent'], tf.float32)\n",
    "        dep = tf.cast(inputs['dependent'], tf.float32)\n",
    "        comp = tf.cast(inputs['compound'], tf.float32)\n",
    "    \n",
    "        # Run the network\n",
    "        ind_emb = self.independent_lstm(ind, training=training)\n",
    "        dep_emb = self.dependent_lstm(dep, training=training)\n",
    "        comp_emb = self.compound_embedding_mlp(comp, training=training)\n",
    "        \n",
    "        x = tf.concat([ind_emb, dep_emb, comp_emb], axis=-1)\n",
    "        \n",
    "        # Apply residual blocks\n",
    "        for block in self.residual_blocks:\n",
    "            residual = x\n",
    "            x = block(x, training=training)\n",
    "            x = tf.nn.leaky_relu(x + residual)  # Skip connection with activation\n",
    "        \n",
    "        # Produce dueling outputs\n",
    "        V = self.value_head(x, training=training) \n",
    "        A = self.advantage_head(x, training=training)    \n",
    "        q = V + (A - tf.reduce_mean(A, axis=-1, keepdims=True))  \n",
    "        return q, network_state\n",
    "\n",
    "    def copy(self, **kwargs):\n",
    "        return type(self)(\n",
    "            self._observation_spec,\n",
    "            self._action_spec,\n",
    "            self._regularization,\n",
    "            self._name,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "class EpsilonDecay():\n",
    "    def __init__(self, schedule, step_counter):\n",
    "        self.schedule = schedule  # A Keras-style schedule (e.g., PolynomialDecay)\n",
    "        self.step_counter = step_counter  # tf.Variable, shared with agent\n",
    "\n",
    "    def __call__(self):\n",
    "        # Read current training step and call the inner schedule\n",
    "        step = tf.cast(self.step_counter, tf.int32)\n",
    "        return self.schedule(step)\n",
    "\n",
    "train_step_counter = tf.Variable(0, dtype=tf.int32, trainable=False)\n",
    "\n",
    "epsilon_schedule_fn = EpsilonDecay(\n",
    "    schedule = PolynomialDecay(\n",
    "        initial_learning_rate=epsilon_start,\n",
    "        decay_steps=num_decay_steps,\n",
    "        end_learning_rate=epsilon_end,\n",
    "        power=2.0,\n",
    "        cycle=False,\n",
    "    ),\n",
    "    step_counter=train_step_counter\n",
    ")  \n",
    "\n",
    "network = CustomNetwork( train_tf_env.observation_spec(), train_tf_env.action_spec(), regularization)\n",
    "fc_schedule = PolynomialDecay(init_fc_lr, num_decay_steps, final_lr, power=3, cycle=False)\n",
    "optimizer = keras.optimizers.legacy.Adam(fc_schedule)\n",
    "\n",
    "# # \"\"\" For Leslie Smith LR Test\"\"\"\n",
    "# S = 3000              # steps per piece\n",
    "# P = int(num_iterations/S)                # number of pieces (segments)\n",
    "# boundaries = [i * S for i in range(1, P)]\n",
    "# #    - values spaced geometrically from eta_start→eta_end\n",
    "# eta_start = 1e-7\n",
    "# eta_end   = 1e-0\n",
    "# values = np.geomspace(eta_start, eta_end, num=P).tolist()\n",
    "\n",
    "# # 3) PIECEWISE SCHEDULE & OPTIMIZER\n",
    "# schedule = PiecewiseConstantDecay(boundaries, values)\n",
    "# optimizer = keras.optimizers.legacy.Adam(schedule)\n",
    "# \"\"\" Test Over\"\"\"\n",
    "\n",
    "agent = DdqnAgent(\n",
    "    train_tf_env.time_step_spec(),\n",
    "    train_tf_env.action_spec(),\n",
    "    q_network = network,\n",
    "    optimizer = optimizer,\n",
    "    epsilon_greedy = epsilon_schedule_fn,\n",
    "    n_step_update = N_step,\n",
    "    target_update_period = target_update_period,\n",
    "    target_update_tau = target_update_tau,\n",
    "    td_errors_loss_fn = common.element_wise_huber_loss,\n",
    "    gamma = gamma,\n",
    "    train_step_counter = train_step_counter,\n",
    "    )\n",
    "\n",
    "agent.initialize()\n",
    "agent._q_network.summary()\n",
    "\n",
    "#Defining Policies\n",
    "random_policy = random_tf_policy.RandomTFPolicy(agent.time_step_spec, agent.action_spec, emit_log_probability = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "158bcbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eta_data = {}\n",
    "\n",
    "# a = S/eval_interval\n",
    "\n",
    "# if not a.is_integer(): ValueError()\n",
    "# else: a = int(a)\n",
    "\n",
    "# for ind,eta in enumerate(values):\n",
    "#     eta_data[eta] = [loss_log[a*ind + a_i] for a_i in range(a)]    \n",
    "    \n",
    "# pprint(eta_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f39fb4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for setting up replay buffer and key positions buffer\n",
    "\n",
    "OBSERVATION_SPEC = train_tf_env.observation_spec()\n",
    "\n",
    "standard_padding_traj = Trajectory(\n",
    "    step_type = tf.fill(dims=(1,), value=StepType.LAST, name=\"step_type\"),\n",
    "    observation = tf.nest.map_structure(\n",
    "                    lambda s: tf.fill(dims = (1,*s.shape), value=0.0),\n",
    "                    OBSERVATION_SPEC\n",
    "                    ),\n",
    "    action = tf.fill(dims=(1,), value=0, name=\"action\"),\n",
    "    policy_info = tf.fill(dims=(1,), value=0.0, name=\"policy_info\"),\n",
    "    next_step_type = tf.fill(dims=(1,), value=StepType.FIRST, name=\"next_step_type\"),\n",
    "    reward = tf.fill(dims=(1,), value=0.0, name=\"reward\"),\n",
    "    discount = tf.fill(dims=(1,), value=1.0, name=\"discount\"),\n",
    ")\n",
    "\n",
    "combined_traj = Trajectory(\n",
    "    *(\n",
    "        tf.nest.map_structure(\n",
    "            lambda x: tf.Variable(tf.zeros((T, *x.shape.as_list()[1:]), dtype=x.dtype), trainable=False),\n",
    "            traj_arg\n",
    "        )\n",
    "        for traj_arg in standard_padding_traj\n",
    "    )\n",
    ")\n",
    "\n",
    "combined_traj_indx = tf.Variable(0, dtype=tf.int32, trainable=False)\n",
    "\n",
    "# combined traj functions\n",
    "@tf.function\n",
    "def reset_combined_traj():\n",
    "    global combined_traj, standard_padding_traj, T\n",
    "    \n",
    "    def _reset_leaf(c_leaf, p_leaf):\n",
    "        c_leaf.assign(tf.zeros(shape=(T, *p_leaf.shape[1:]), dtype=p_leaf.dtype))\n",
    "        \n",
    "    \"\"\"Reset the global combined_traj to zeros.\"\"\"\n",
    "    tf.nest.map_structure(_reset_leaf, combined_traj, standard_padding_traj)\n",
    "\n",
    "@tf.function\n",
    "def combined_traj_write(write_traj: Trajectory, index: tf.Tensor):\n",
    "    \"\"\"\n",
    "    Writes a single trajectory into combined_traj at given index,\n",
    "    handling mismatched policy_info structure.\n",
    "    \"\"\"\n",
    "    global combined_traj\n",
    "    def _assign_field(c_field, w_field):\n",
    "        c_field.assign(\n",
    "            tf.tensor_scatter_nd_update(c_field, [[index]], w_field)\n",
    "        )\n",
    "\n",
    "    # Make a mutable copy of write_traj\n",
    "    w_traj = write_traj\n",
    "\n",
    "    # Special-case policy_info to match combined_traj\n",
    "    if isinstance(w_traj.policy_info, tuple) and not hasattr(w_traj.policy_info, '_fields'):\n",
    "        # Empty tuple case → use zeros of the same shape as combined_traj.policy_info\n",
    "        w_traj = w_traj._replace(\n",
    "            policy_info=tf.zeros(shape=(1,),dtype=tf.float32)\n",
    "        )\n",
    "    elif hasattr(w_traj.policy_info, '_fields'):  \n",
    "        # Namedtuple case → pick the first/primary field (here log_probability)\n",
    "        w_traj = w_traj._replace(\n",
    "            policy_info=w_traj.policy_info.log_probability\n",
    "        )\n",
    "\n",
    "    tf.nest.map_structure(_assign_field, combined_traj, w_traj)\n",
    "\n",
    "@tf.function\n",
    "def fifo_shift_combined_traj():\n",
    "    global combined_traj, standard_padding_traj\n",
    "\n",
    "    def _shift_assign(c_leaf, p_leaf):\n",
    "        # c_leaf: (T, ...) trajectory tensor (tf.Variable)\n",
    "        # p_leaf: (1, ...) padding tensor\n",
    "        new_val = tf.concat([c_leaf[1:], p_leaf], axis=0)\n",
    "        c_leaf.assign(new_val)  # In-place update\n",
    "\n",
    "    # Applies to all leaves in nested structure:\n",
    "    tf.nest.map_structure(_shift_assign, combined_traj, standard_padding_traj)\n",
    "        \n",
    "batched_traj = Trajectory(\n",
    "    *(\n",
    "        tf.nest.map_structure(\n",
    "            lambda x: tf.Variable(tf.zeros( shape = (max_window_size, *x.shape), dtype=x.dtype),\n",
    "                                  name=x.name,\n",
    "                                  trainable = False),\n",
    "            traj_arg\n",
    "        )\n",
    "      \n",
    "        for traj_arg in combined_traj\n",
    "    )\n",
    ")\n",
    "batched_traj_indx = tf.Variable(0, dtype=tf.int32, trainable=False)\n",
    "\n",
    "@tf.function\n",
    "def batched_traj_write(write_traj: Trajectory, index: tf.Tensor):\n",
    "    \"\"\"\n",
    "    Update batched_traj at position `index` from write_traj, for all fields.\n",
    "    Works with nested structures inside the trajectory.\n",
    "    \"\"\"\n",
    "    def _assign_field(batched_field, write_field):\n",
    "        update = tf.expand_dims(write_field, axis=0)  # add batch dim\n",
    "        batched_field.assign(\n",
    "            tf.tensor_scatter_nd_update(batched_field, [[index]], update)\n",
    "        )\n",
    "\n",
    "    tf.nest.map_structure(_assign_field, batched_traj, write_traj)\n",
    "\n",
    "def add_to_per_buffer(*flat_list: list):\n",
    "    global per_replay_buffer\n",
    "    # Number of trajectories in this batch\n",
    "    td_err = flat_list[-1]\n",
    "    batch_size = td_err.size  # same as td_err.shape[0]\n",
    "    \n",
    "    batched_traj = tf.nest.pack_sequence_as(standard_padding_traj, flat_list[:-1])\n",
    "    tuple_list = []\n",
    "    for i in range(batch_size):\n",
    "        # extract i-th example from each leaf (works for nested obs)\n",
    "        single_traj = tf.nest.map_structure(lambda x: x[i], batched_traj)\n",
    "        tuple_list.append((single_traj, float(td_err[i])))\n",
    "        \n",
    "    per_replay_buffer.add_batch_experience(tuple_list)\n",
    "    return None\n",
    "\n",
    "# Tests for combined traj\n",
    "# test_traj = Trajectory(*[\n",
    "#     tf.ones_like(field) * (1 if field.dtype.is_integer else 5.0)\n",
    "#     for field in standard_padding_traj\n",
    "# ])\n",
    "# combined_traj_indx.assign(tf.constant(1))\n",
    "# print(f\"Test Traj {test_traj}\")\n",
    "# print(f\"\\n\\n\\n\\nOriginal:\\n{combined_traj}\")\n",
    "# combined_traj_write(test_traj,combined_traj_indx)\n",
    "# print(f\"\\n\\n\\n\\nAfter write:\\n{combined_traj}\")\n",
    "# fifo_shift_combined_traj()\n",
    "# print(f\"\\n\\n\\n\\nAfter fifo shift:\\n{combined_traj}\")\n",
    "# reset_combined_traj()\n",
    "# print(f\"\\n\\n\\n\\nAfter reset:\\n{combined_traj}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5a542c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tl/zs0yvsgj7bjfb08xkfwq3nbc0000gn/T/__autograph_generated_filelasyf_j7.py:14: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  retval_ = ag__.and_(lambda : ag__.ld(state) is not None, lambda : ag__.and_(lambda : ag__.ld(state) is not (), lambda : ag__.ld(state) is not []))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: PER Buffer Capacity:32768, PER Actual Trajectories:32680, Batch Size: 128\n",
      "Fold 1: PER Buffer Capacity:32768, PER Actual Trajectories:32768, Batch Size: 128\n",
      "Fold 2: PER Buffer Capacity:32768, PER Actual Trajectories:32768, Batch Size: 128\n",
      "Fold 3: PER Buffer Capacity:32768, PER Actual Trajectories:32768, Batch Size: 128\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create the PER buffer and allocate (trajectory,td_err) to it. \n",
    "\"\"\"\n",
    "\n",
    "# Create and fill out PER buffers \n",
    "per_buffer_list = [PrioritizedExperienceReplayBuffer(per_replay_capacity, per_replay_alpha, per_replay_beta) for _ in range(num_folds)]\n",
    "\n",
    "# IMPORTANT: the observer and consequently the drivers act on this buffe, so always assign it\n",
    "per_replay_buffer: PrioritizedExperienceReplayBuffer\n",
    "\n",
    "# Observer to add collected trajectories to the replay buffer\n",
    "reset_combined_traj() # all boundary trajectories\n",
    "combined_traj_indx.assign(0) # reset index to 0\n",
    "batched_traj_indx.assign(0) # reset batch traj\n",
    "\n",
    "@tf.function\n",
    "def per_buffer_observer(traj: Trajectory):\n",
    "    global agent, gamma, combined_traj, combined_traj_indx, T, batched_traj_indx, batched_traj, td_error_tnsr\n",
    "    \n",
    "    traj = traj.replace(reward = traj.reward[\"training\"]) # for training purposes, use only the training reward\n",
    "    combined_traj_write(traj, combined_traj_indx)\n",
    "    # post increment means that the index is now equal to the size of the valid entries in combined trajectories\n",
    "    combined_traj_indx.assign_add(1) \n",
    "    \n",
    "    if traj.is_boundary():\n",
    "        # Process ALL possible trajectories in the buffer\n",
    "        for _ in tf.range( combined_traj_indx - 1):\n",
    "            # 2. Add to batch BEFORE checking boundary\n",
    "            \n",
    "            batched_traj_write(combined_traj, batched_traj_indx)\n",
    "            batched_traj_indx.assign_add(1)\n",
    "            \n",
    "            # 3. Check if FIRST element is LAST AFTER processing\n",
    "            if tf.equal(combined_traj.step_type[0], StepType.LAST):\n",
    "                tf.print(\"The first trajectory is of step type: \",combined_traj.next_step_type[0],\"\\ncombined_traj: \",combined_traj)\n",
    "\n",
    "            fifo_shift_combined_traj()\n",
    "                    \n",
    "        # compute td_errs \n",
    "        # reward_scale_factor is at environment level\n",
    "        # loss function does not matter, since only td_errors are needed\n",
    "        \n",
    "        sample_traj = tf.nest.map_structure(lambda x, ind = batched_traj_indx: x[0:ind], batched_traj) # from 0 to ind-1, becasue of slicing\n",
    "        \n",
    "        loss_info = agent._loss(sample_traj, gamma=gamma, td_errors_loss_fn=common.element_wise_huber_loss,\n",
    "                                reward_scale_factor=1.0, weights=None, training=True)\n",
    "        td_err = loss_info.extra.td_error\n",
    "        \n",
    "        # add batch to buffer and reset batch\n",
    "        tf.numpy_function(\n",
    "            add_to_per_buffer,\n",
    "            inp=[ *tf.nest.flatten(sample_traj) , td_err],\n",
    "            Tout=[]  # Match empty list if no returns needed\n",
    "        )\n",
    "        batched_traj_indx.assign(0)\n",
    "        \n",
    "        # reset combined traj and states\n",
    "        reset_combined_traj()\n",
    "        combined_traj_indx.assign(0)\n",
    "\n",
    "    # Handle regular sliding window case\n",
    "    if combined_traj_indx == T:\n",
    "        \n",
    "        batched_traj_write(combined_traj, batched_traj_indx)\n",
    "        batched_traj_indx.assign_add(1)\n",
    "        \n",
    "        fifo_shift_combined_traj()\n",
    "        combined_traj_indx.assign_sub(1)\n",
    "\n",
    "# Create the dynamic step driver\n",
    "initial_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "    env=train_tf_env,                         # The environment to collect data from\n",
    "    policy=random_policy,                     # Policy used to collect data (e.g., random_policy)\n",
    "    observers=[per_buffer_observer],# Observer that adds to the replay buffer\n",
    "    num_steps=per_collect_steps           # Number of steps to collect\n",
    ")\n",
    "\n",
    "# Run the driver\n",
    "initial_driver.run = common.function(initial_driver.run)\n",
    "\n",
    "# Populate all buffers\n",
    "for index in range(num_folds):\n",
    "    fold_index = index\n",
    "    train_py_env.update_data_cache(folds[fold_index].train, folds[fold_index].files_train)\n",
    "    train_py_env.reset()\n",
    "    per_replay_buffer = per_buffer_list[fold_index]\n",
    "    \n",
    "    initial_driver.run(train_tf_env.reset())\n",
    "\n",
    "\"\"\"\n",
    "Print the number of actual elements added to the buffers\n",
    "\"\"\"\n",
    "for index in range(num_folds):\n",
    "    print(f\"Fold {index}: PER Buffer Capacity:{per_buffer_list[index].capacity}, PER Actual Trajectories:{per_buffer_list[index].length}, Batch Size: {num_per_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78b636ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for interacting with python native per buffer and evalution of key metrics\n",
    "@tf.function\n",
    "def evaluate_episode_metrics(env, num_episodes, policy):\n",
    "    \"\"\"\n",
    "    Runs `num_episodes` episodes and returns:\n",
    "      avg_return:        average training return per episode (divided by reward_scalar)\n",
    "      avg_money_pct:     average evaluation money per episode as a percentage (money *100 / reward_scalar)\n",
    "      avg_trades:        average trades per episode (buy+sell counted as one)\n",
    "    \"\"\"\n",
    "    # capture reward scalar as a TF constant\n",
    "    reward_sf = tf.constant(reward_scalar, dtype=tf.float32)\n",
    "\n",
    "    # outer loop body\n",
    "    def _episode_body(i, total_train, total_eval, total_trades):\n",
    "        # reset env & policy state\n",
    "        ts = env.reset()\n",
    "        policy_state = policy.get_initial_state(batch_size=1)\n",
    "\n",
    "        # per-episode accumulators\n",
    "        ep_train = tf.constant(0.0, dtype=tf.float32)\n",
    "        ep_eval = tf.constant(0.0, dtype=tf.float32)\n",
    "        ep_trades = tf.constant(0, dtype=tf.int32)\n",
    "\n",
    "        # inner loop condition / body\n",
    "        def _step_cond(ts, policy_state, ep_train, ep_eval, ep_trades):\n",
    "            return tf.logical_not(ts.is_last())\n",
    "\n",
    "        def _step_body(ts, policy_state, ep_train, ep_eval, ep_trades):\n",
    "            action_step = policy.action(ts, policy_state)\n",
    "            ts_next = env.step(action_step.action)\n",
    "\n",
    "            # extract training / evaluation components from the (structured) reward\n",
    "            # reduce_sum collapses potential (1,) batch dimension to scalar\n",
    "            train_r = tf.cast(tf.reduce_sum(ts_next.reward[\"training\"]), tf.float32)\n",
    "            eval_r = tf.cast(tf.reduce_sum(ts_next.reward[\"evaluation\"]), tf.float32)\n",
    "\n",
    "            ep_train = ep_train + train_r\n",
    "            ep_eval = ep_eval + eval_r\n",
    "\n",
    "            # increment trade count when evaluation (money) is non-zero\n",
    "            delta = tf.cond(tf.not_equal(eval_r, 0.0),\n",
    "                            lambda: tf.constant(1, dtype=tf.int32),\n",
    "                            lambda: tf.constant(0, dtype=tf.int32))\n",
    "            ep_trades = ep_trades + delta\n",
    "\n",
    "            return ts_next, action_step.state, ep_train, ep_eval, ep_trades\n",
    "\n",
    "        # run episode\n",
    "        _, _, ep_train, ep_eval, ep_trades = tf.while_loop(\n",
    "            _step_cond,\n",
    "            _step_body,\n",
    "            [ts, policy_state, ep_train, ep_eval, ep_trades],\n",
    "            # optional shape_invariants could be added if needed\n",
    "        )\n",
    "\n",
    "        # accumulate totals and increment episode index\n",
    "        return i + 1, total_train + ep_train, total_eval + ep_eval, total_trades + ep_trades\n",
    "\n",
    "    # initialize outer loop vars\n",
    "    i0 = tf.constant(0, dtype=tf.int32)\n",
    "    total_train0 = tf.constant(0.0, dtype=tf.float32)\n",
    "    total_eval0 = tf.constant(0.0, dtype=tf.float32)\n",
    "    total_trades0 = tf.constant(0, dtype=tf.int32)\n",
    "\n",
    "    # outer loop\n",
    "    _, total_train, total_eval, total_trades = tf.while_loop(\n",
    "        lambda i, *_: tf.less(i, num_episodes),\n",
    "        _episode_body,\n",
    "        [i0, total_train0, total_eval0, total_trades0],\n",
    "    )\n",
    "\n",
    "    # compute averages:\n",
    "    n_eps_f = tf.cast(num_episodes, tf.float32)\n",
    "    avg_return = total_train / (n_eps_f * reward_sf)                    # training return normalized by reward scalar\n",
    "    avg_money_pct = (total_eval * 100.0) / (n_eps_f * reward_sf)         # evaluation money -> percent\n",
    "    avg_trades = tf.cast(total_trades, tf.float32) / (2.0 * n_eps_f)    # buy+sell counted as one\n",
    "\n",
    "    return avg_return, avg_money_pct, avg_trades\n",
    "\n",
    "\"\"\" Helper functions dor interacting with native per buffer\"\"\"\n",
    "def sample_helper(batch_size):\n",
    "    trajs, weights, idxs = per_replay_buffer.sample(batch_size)\n",
    "    \n",
    "        # Create batched trajectory by stacking each component\n",
    "    batched_traj = tf.nest.map_structure(lambda *x: np.stack(x, axis = 0), *trajs)\n",
    "    flat_traj = tf.nest.flatten(batched_traj)\n",
    "    return flat_traj + [np.array(weights, dtype=np.float32), np.array(idxs,dtype = np.int32)]\n",
    "    \n",
    "tout = []\n",
    "tf.nest.map_structure(lambda x: tout.append(x.dtype), standard_padding_traj) # dtypes for trajectory\n",
    "tout.append(tf.float32) # importance_weights dtype\n",
    "tout.append(tf.int32)      # index dtype\n",
    "\n",
    "@tf.function\n",
    "def get_per_batch(batch_size) -> Tuple[Trajectory, tf.Tensor, tf.Tensor]:\n",
    "    \"\"\"\n",
    "    Fetch a batch from the native PER helper and return fully-shaped tensors.\n",
    "\n",
    "    Args:\n",
    "        batch_size: scalar Python int or 0-D `tf.Tensor` (int32/int64) giving the requested batch size.\n",
    "\n",
    "    Returns:\n",
    "        all_trajs: `Trajectory`-shaped nested structure whose leaves are `tf.Tensor`s with runtime\n",
    "                   shape `[batch_size, ...]` (tail dims match `combined_traj` template).\n",
    "        weights:   `tf.Tensor` of shape `[batch_size]`, dtype `tf.float32` (importance weights).\n",
    "        idxs:      `tf.Tensor` of shape `[batch_size]`, dtype `tf.int32` (buffer indices).\n",
    "\n",
    "    Notes:\n",
    "        - This function calls a Python-side sampling helper via `tf.numpy_function` and repacks\n",
    "          the flattened results into the nested `combined_traj` structure, then reshapes leaves\n",
    "          so the leading dimension equals `batch_size`.\n",
    "    \"\"\"\n",
    "    global tout, combined_traj\n",
    "    \n",
    "    flat_list = tf.numpy_function( func=sample_helper, inp=[batch_size], Tout=tout)\n",
    "    idxs = flat_list[-1]\n",
    "    weights = flat_list[-2]\n",
    "    \n",
    "    all_trajs = tf.nest.pack_sequence_as(combined_traj, flat_list[:-2])\n",
    "    all_trajs = tf.nest.map_structure(lambda x, c: tf.reshape(x , [batch_size, *c.shape]) ,all_trajs, combined_traj)\n",
    "    \n",
    "    weights = tf.reshape(weights, [batch_size])\n",
    "    idxs = tf.reshape(idxs, [batch_size])\n",
    "    \n",
    "    return all_trajs, weights, idxs\n",
    "\n",
    "def per_set_beta(beta: float |np.ndarray): \n",
    "    global per_replay_buffer\n",
    "    per_replay_buffer.set_beta(float(beta))\n",
    "    return None\n",
    "\n",
    "def per_update_priorities(per_indexes, td_errors):\n",
    "    global per_replay_buffer\n",
    "    per_replay_buffer.update_leaf_priorities(per_indexes, td_errors)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a3a1451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 500 : loss = 1.40867877\n",
      "step = 1000 : loss = 0.493961513\n",
      "step = 1500 : loss = 0.48890686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tl/zs0yvsgj7bjfb08xkfwq3nbc0000gn/T/__autograph_generated_filelosq8de0.py:74: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  ag__.if_stmt(ag__.ld(policy_state) is (), if_body_2, else_body_2, get_state_2, set_state_2, ('do_return', 'retval_'), 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 1500: Validation: Count = 4.00000, Money = 0.09643, Reward = -0.00035\n",
      "step = 1500: Test: Count = 2.00000, Money = 0.16132, Reward = 0.00021\n",
      "step = 2000 : loss = 0.217109442\n",
      "step = 2500 : loss = 0.596340358\n",
      "step = 3000 : loss = 0.463815629\n",
      "step = 3000: Validation: Count = 0.00000, Money = 0.00000, Reward = 0.00000\n",
      "step = 3000: Test: Count = 0.00000, Money = 0.00000, Reward = 0.00000\n",
      "step = 3500 : loss = 0.595168114\n",
      "step = 4000 : loss = 0.456965148\n",
      "step = 4500 : loss = 0.622768402\n",
      "step = 4500: Validation: Count = 1.50000, Money = -0.13857, Reward = -0.00104\n",
      "step = 4500: Test: Count = 1.50000, Money = -0.12454, Reward = -0.00015\n",
      "step = 5000 : loss = 0.291768432\n",
      "step = 5500 : loss = 0.280453622\n",
      "step = 6000 : loss = 0.490948588\n",
      "step = 6000: Validation: Count = 2.00000, Money = 3.54627, Reward = 0.03439\n",
      "step = 6000: Test: Count = 2.00000, Money = -0.13064, Reward = -0.00037\n",
      "step = 6500 : loss = 0.335695982\n",
      "step = 7000 : loss = 0.503024101\n",
      "step = 7500 : loss = 0.364633381\n",
      "step = 7500: Validation: Count = 1.50000, Money = 0.07143, Reward = -0.00175\n",
      "step = 7500: Test: Count = 1.00000, Money = -0.00771, Reward = -0.00042\n",
      "step = 8000 : loss = 0.342697442\n",
      "step = 8500 : loss = 0.331009537\n",
      "step = 9000 : loss = 0.339914352\n",
      "step = 9000: Validation: Count = 0.00000, Money = 0.00000, Reward = 0.00000\n",
      "step = 9000: Test: Count = 0.00000, Money = 0.00000, Reward = 0.00000\n",
      "step = 9500 : loss = 0.466914505\n",
      "step = 10000 : loss = 0.332224965\n",
      "step = 10500 : loss = 0.340079606\n",
      "step = 10500: Validation: Count = 1.00000, Money = 0.30697, Reward = 0.00344\n",
      "step = 10500: Test: Count = 1.00000, Money = 0.04506, Reward = -0.00038\n",
      "step = 11000 : loss = 0.327845126\n",
      "step = 11500 : loss = 0.441544116\n",
      "step = 12000 : loss = 0.257190138\n",
      "step = 12000: Validation: Count = 0.00000, Money = 0.00000, Reward = 0.00000\n",
      "step = 12000: Test: Count = 0.00000, Money = 0.00000, Reward = 0.00000\n",
      "step = 12500 : loss = 0.390528023\n",
      "step = 13000 : loss = 0.33134234\n",
      "step = 13500 : loss = 0.281739205\n",
      "step = 13500: Validation: Count = 0.00000, Money = 0.00000, Reward = 0.00000\n",
      "step = 13500: Test: Count = 0.00000, Money = 0.00000, Reward = 0.00000\n",
      "step = 14000 : loss = 0.566349506\n",
      "step = 14500 : loss = 0.322289407\n",
      "step = 15000 : loss = 0.311357349\n",
      "step = 15000: Validation: Count = 2.50000, Money = -0.45938, Reward = -0.00103\n",
      "step = 15000: Test: Count = 0.00000, Money = 0.00000, Reward = 0.00000\n",
      "step = 15500 : loss = 0.482097864\n",
      "step = 16000 : loss = 0.233404696\n",
      "step = 16500 : loss = 0.287541389\n",
      "step = 16500: Validation: Count = 0.00000, Money = 0.00000, Reward = 0.00000\n",
      "step = 16500: Test: Count = 0.00000, Money = 0.00000, Reward = 0.00000\n",
      "step = 17000 : loss = 0.618001103\n",
      "step = 17500 : loss = 0.425173402\n",
      "step = 18000 : loss = 0.445593178\n",
      "step = 18000: Validation: Count = 0.00000, Money = 0.00000, Reward = 0.00000\n",
      "step = 18000: Test: Count = 0.00000, Money = 0.00000, Reward = 0.00000\n",
      "step = 18500 : loss = 0.513740063\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 142>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m             training_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    140\u001b[0m             evaluation_step(step)\n\u001b[0;32m--> 142\u001b[0m \u001b[43mtraining_and_evalution\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mtraining_and_evalution\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iterations):\n\u001b[1;32m    115\u001b[0m     step \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mtrain_step_counter\n\u001b[0;32m--> 116\u001b[0m     loss, curr_step, policy_state \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurr_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(step\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m num_fold_update_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m    119\u001b[0m         \n\u001b[1;32m    120\u001b[0m         \u001b[38;5;66;03m# update training and validation environments\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/programming/Python/StockMarket/.venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Documents/programming/Python/StockMarket/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Documents/programming/Python/StockMarket/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:877\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 877\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    881\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    882\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/programming/Python/StockMarket/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/programming/Python/StockMarket/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Documents/programming/Python/StockMarket/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m   \u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Documents/programming/Python/StockMarket/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/programming/Python/StockMarket/.venv/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m~/Documents/programming/Python/StockMarket/.venv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Preparation for training.\n",
    "agent.train = common.function(agent._train)\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Collection happens with the collect_policy\n",
    "collect_driver = dynamic_step_driver.DynamicStepDriver(\n",
    "  env = train_tf_env,                 \n",
    "  policy = agent.collect_policy,                \n",
    "  observers =[per_buffer_observer],\n",
    "  num_steps = collect_steps_per_iteration    \n",
    ")\n",
    "collect_driver.run = common.function(collect_driver.run)\n",
    "\n",
    "# Evaluate the agent's policy for plotting purposes.\n",
    "training_loss = [] # Store total loss values\n",
    "training_q_values = [] # Storage for Q-values at evaluation intervals\n",
    "\n",
    "validation_return = []\n",
    "validation_money = []\n",
    "validation_count = []\n",
    "\n",
    "test_return = []\n",
    "test_money = [] # stores the literal money \n",
    "test_count = [] # stores the number of trades an agent makes\n",
    "\n",
    "per_replay_beta_tensor = tf.Variable(per_replay_beta, dtype=tf.float32)\n",
    "\n",
    "@tf.function\n",
    "def train_step(step, curr_step, policy_state):\n",
    "    global per_replay_beta_tensor, num_per_samples\n",
    "    \n",
    "    # Collect experience.\n",
    "    curr_step, policy_state = collect_driver.run(curr_step, policy_state)\n",
    "\n",
    "    # Sample from PER buffer\n",
    "    all_trajs, all_is_weights, per_indexes = get_per_batch(num_per_samples)\n",
    "    \n",
    "    # Train agent\n",
    "    loss_info = agent.train(all_trajs, all_is_weights)\n",
    "    \n",
    "    total_loss = loss_info.loss\n",
    "    td_errors = loss_info.extra.td_error\n",
    "    \n",
    "    # Update PER priorities and beta\n",
    "    tf.numpy_function(func = per_update_priorities,inp=[per_indexes, td_errors],Tout=[])\n",
    "    \n",
    "    per_replay_beta_tensor.assign(tf.minimum(per_replay_beta_tensor + per_delta_beta, 1.0))\n",
    "    \n",
    "    tf.numpy_function(func = per_set_beta,inp=[per_replay_beta_tensor],Tout=[])\n",
    "        \n",
    "    if step % log_interval == 0:\n",
    "        tf.print(\"step =\", step, \": loss =\", total_loss)\n",
    "    \n",
    "    return total_loss, curr_step, policy_state\n",
    "    \n",
    "def evaluation_step(step):\n",
    "    \"\"\" Validation Metrics \"\"\"\n",
    "    avg_return_t, avg_money_pct_t, avg_count_t = evaluate_episode_metrics(validation_tf_env, num_eval_episodes, agent.policy)\n",
    "\n",
    "    # convert to python floats and append (avg_money_pct_t is already in percent)\n",
    "    avg_return = float(avg_return_t.numpy())\n",
    "    validation_return.append(avg_return)\n",
    "\n",
    "    avg_money_pct = float(avg_money_pct_t.numpy())   # already *100 inside evaluate_episode_metrics\n",
    "    validation_money.append(avg_money_pct)\n",
    "\n",
    "    avg_count = float(avg_count_t.numpy())\n",
    "    validation_count.append(avg_count)\n",
    "\n",
    "    print(f\"step = {step}: Validation: Count = {avg_count:.5f}, Money = {avg_money_pct:.5f}, Reward = {avg_return:.5f}\")\n",
    "\n",
    "    \"\"\" Test Metrics \"\"\"\n",
    "    avg_return_t, avg_money_pct_t, avg_count_t = evaluate_episode_metrics(test_tf_env, num_eval_episodes, agent.policy)\n",
    "\n",
    "    avg_return = float(avg_return_t.numpy())\n",
    "    test_return.append(avg_return)\n",
    "\n",
    "    avg_money_pct = float(avg_money_pct_t.numpy())   # already *100 inside evaluate_episode_metrics\n",
    "    test_money.append(avg_money_pct)\n",
    "\n",
    "    avg_count = float(avg_count_t.numpy())\n",
    "    test_count.append(avg_count)\n",
    "\n",
    "    print(f\"step = {step}: Test: Count = {avg_count:.5f}, Money = {avg_money_pct:.5f}, Reward = {avg_return:.5f}\")\n",
    "\n",
    "    \"\"\" Training Metrics \"\"\"\n",
    "    # === Collect Q-values for randomly selected states ===\n",
    "    num_q_samples = min(num_per_samples, 10)\n",
    "    trajs, _, _= get_per_batch(num_q_samples)\n",
    "\n",
    "    # slice out the first time‐step before calling the network\n",
    "    q_values, _ = agent._q_network(\n",
    "        tf.nest.map_structure(lambda x: x[:,0], trajs.observation) , #remove T (N step update part)\n",
    "        tf.nest.map_structure(lambda x: x[:,0], trajs.step_type),   # input T = 0\n",
    "        network_state=agent.policy.get_initial_state(num_q_samples),\n",
    "        training=False\n",
    "    ) # Out [B, A]\n",
    "    # average over batch\n",
    "    q_values = tf.reduce_mean(q_values, axis=0)  # → [A,]\n",
    "    training_q_values.append(q_values.numpy())\n",
    "    \n",
    "def training_and_evalution():\n",
    "    \n",
    "    global agent, eval_interval, num_fold_update_interval, fold_index, folds, num_folds\n",
    "    global combined_traj_indx, batched_traj_indx, per_replay_buffer \n",
    "    \n",
    "    reset_combined_traj() # all boundary trajectories\n",
    "    combined_traj_indx.assign(0) # reset index to 0\n",
    "    batched_traj_indx.assign(0) # reset batch\n",
    "\n",
    "    curr_step = train_tf_env.reset()# Reset the environment.\n",
    "    policy_state = None\n",
    "    \n",
    "    for _ in range(num_iterations):\n",
    "        step = agent.train_step_counter\n",
    "        loss, curr_step, policy_state = train_step(step, curr_step, policy_state)\n",
    "        step = int(step.numpy())\n",
    "        if step % num_fold_update_interval == 0: \n",
    "            \n",
    "            # update training and validation environments\n",
    "            train_py_env.update_data_cache( folds[fold_index].train, folds[fold_index].files_train )\n",
    "            validation_py_env.update_data_cache(folds[fold_index].validation, folds[fold_index].files_validation)\n",
    "            \n",
    "            # reset states\n",
    "            train_py_env.reset()\n",
    "            curr_step = train_tf_env.reset()\n",
    "            policy_state = None\n",
    "            \n",
    "            # reset buffer parameters\n",
    "            reset_combined_traj()\n",
    "            combined_traj_indx.assign(0)\n",
    "            batched_traj_indx.assign(0)\n",
    "            \n",
    "            # use another buffer\n",
    "            per_replay_buffer = per_buffer_list[fold_index]\n",
    "            fold_index = (fold_index + 1) % num_folds\n",
    "            \n",
    "        if step % eval_interval == 0:\n",
    "            training_loss.append(loss.numpy())\n",
    "            evaluation_step(step)\n",
    "\n",
    "training_and_evalution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c49179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# assemble arrays\n",
    "q_vals = np.array(training_q_values)\n",
    "q_mean = np.mean(q_vals, axis=1)\n",
    "q_max  = np.max(q_vals, axis=1)\n",
    "\n",
    "data = {\n",
    "    'loss': np.array(training_loss),\n",
    "    'q_mean': q_mean,\n",
    "    'q_max': q_max,\n",
    "    'val_return': np.array(validation_return),\n",
    "    'val_money': np.array(validation_money),\n",
    "    'val_count': np.array(validation_count),\n",
    "    'test_return': np.array(test_return),\n",
    "    'test_money': np.array(test_money),\n",
    "    'test_count': np.array(test_count),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "# df = pd.read_csv(\"../temp.csv\", index_col=False, header=0)\n",
    "# Pearson correlation\n",
    "corr = df.corr(method='pearson').abs()\n",
    "\n",
    "# # Heatmap\n",
    "plt.figure(figsize=(9,7))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ea4888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- plotting (reorganized middle & bottom plots) ---\n",
    "# prepare arrays (unchanged)\n",
    "q_values_array = np.array(training_q_values)            # [num_eval_points, num_actions]\n",
    "val_return_arr   = np.array(validation_return)\n",
    "val_money_arr    = np.array(validation_money)\n",
    "val_count_arr    = np.array(validation_count)\n",
    "\n",
    "test_return_arr  = np.array(test_return)\n",
    "test_money_arr   = np.array(test_money)\n",
    "test_count_arr   = np.array(test_count)\n",
    "\n",
    "steps = list(range(0, num_iterations, eval_interval))\n",
    "\n",
    "# Compute mean and max Q over actions\n",
    "q_mean = np.mean(q_values_array, axis=1)\n",
    "q_max  = np.max(q_values_array, axis=1)\n",
    "\n",
    "# Create figure with three subplots stacked vertically\n",
    "fig, (ax_top, ax_mid, ax_bottom) = plt.subplots(nrows=3, figsize=(14, 14), sharex=True)\n",
    "\n",
    "# --- Top Plot: Q-values and Loss (unchanged) ---\n",
    "ax_top.set_ylabel('Loss', color='black')\n",
    "line_loss = ax_top.plot(steps, training_loss, label='Loss', color='black', linewidth=2, linestyle='dashed')[0]\n",
    "ax_top.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "ax2_top = ax_top.twinx()\n",
    "ax2_top.set_ylabel('Q Values', color='tab:red')\n",
    "\n",
    "q_line_short     = ax2_top.plot(steps, q_values_array[:, 0], color='tab:cyan', linestyle='dotted', linewidth=1.2, label='Short')[0]\n",
    "q_line_no_trade  = ax2_top.plot(steps, q_values_array[:, 1], color='tab:red',  linestyle='dotted', linewidth=1.2, label='NoTrade')[0]\n",
    "q_line_long      = ax2_top.plot(steps, q_values_array[:, 2], color='tab:green',linestyle='dotted', linewidth=1.2, label='Long')[0]\n",
    "\n",
    "q_line_mean = ax2_top.plot(steps, q_mean, label='Mean Q', color='#6a3d9a', linewidth=2)[0]\n",
    "q_line_max  = ax2_top.plot(steps, q_max,  label='Max Q',  color='#ff7f00', linewidth=2)[0]\n",
    "\n",
    "ax2_top.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "handles_top = [line_loss, q_line_mean, q_line_max, q_line_short, q_line_long, q_line_no_trade]\n",
    "labels_top = [h.get_label() for h in handles_top]\n",
    "ax_top.legend(handles_top, labels_top, loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=4, fontsize=9)\n",
    "ax_top.set_title('Q-Values and Loss Over Time')\n",
    "\n",
    "# --- Middle Plot: Average Return (Validation + Test) ---\n",
    "ax_mid.set_xlabel('Step')\n",
    "ax_mid.set_ylabel('Average Return', color='tab:blue')\n",
    "# Both lines blue but different structured lines\n",
    "val_line_ret = ax_mid.plot(steps, val_return_arr, label='Val Avg Return', color='tab:blue', linewidth=2, linestyle='solid')[0]\n",
    "test_line_ret = ax_mid.plot(steps, test_return_arr, label='Test Avg Return', color='tab:blue', linewidth=2, linestyle='dashed')[0]\n",
    "ax_mid.tick_params(axis='y', labelcolor='tab:blue')\n",
    "ax_mid.axhline(0, color='skyblue', linestyle='dashed', linewidth=1.2, label='Return=0')\n",
    "\n",
    "handles_mid = [val_line_ret, test_line_ret]\n",
    "labels_mid = [h.get_label() for h in handles_mid]\n",
    "ax_mid.legend(handles_mid, labels_mid, loc='upper center', bbox_to_anchor=(0.5, -0.18), ncol=2, fontsize=9)\n",
    "ax_mid.set_title('Average Return: Validation vs Test')\n",
    "\n",
    "# --- Bottom Plot: Money Made (as percentage label) and Counts (Both Validation + Test) ---\n",
    "ax_bottom.set_xlabel('Step')\n",
    "ax_bottom.set_ylabel(f'Percentage Money Made for {check_ticker}', color='tab:green')\n",
    "# Money lines share the same money axis (green) — legend labels mention Val/Test only\n",
    "val_line_money = ax_bottom.plot(steps, val_money_arr, label='Val Money (%)', color='tab:green', linewidth=2, linestyle='solid')[0]\n",
    "test_line_money = ax_bottom.plot(steps, test_money_arr, label='Test Money (%)', color='tab:green', linewidth=2, linestyle='dashed')[0]\n",
    "ax_bottom.tick_params(axis='y', labelcolor='tab:green')\n",
    "ax_bottom.axhline(0, color='lightgreen', linestyle='dashed', linewidth=1.2, label='Money=0')\n",
    "\n",
    "# Counts on a separate right axis\n",
    "ax2_bottom = ax_bottom.twinx()\n",
    "ax2_bottom.spines['right'].set_position(('outward', 60))\n",
    "ax2_bottom.set_ylabel('Counts', color='tab:orange')\n",
    "val_line_counts = ax2_bottom.plot(steps, val_count_arr, label='Val Counts', color='tab:orange', linewidth=2, linestyle='solid')[0]\n",
    "test_line_counts = ax2_bottom.plot(steps, test_count_arr, label='Test Counts', color='tab:orange', linewidth=2, linestyle='dotted')[0]\n",
    "ax2_bottom.tick_params(axis='y', labelcolor='tab:orange')\n",
    "\n",
    "# Combined legend (money & counts). Money legend entries do not include $check_ticker text.\n",
    "handles_bottom = [val_line_money, test_line_money, val_line_counts, test_line_counts]\n",
    "labels_bottom = [h.get_label() for h in handles_bottom]\n",
    "ax_bottom.legend(handles_bottom, labels_bottom, loc='upper center', bbox_to_anchor=(0.5, -0.25), ncol=4, fontsize=9)\n",
    "ax_bottom.set_title('Money Made (Val/Test) and Counts Over Time')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb4f7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_dir = \"../Agent\"\n",
    "\n",
    "# === Save Agent ===\n",
    "checkpoint = tf.train.Checkpoint(agent=agent, optimizer=agent._optimizer)\n",
    "checkpoint_manager = tf.train.CheckpointManager(checkpoint, agent_dir, max_to_keep=1)\n",
    "checkpoint_manager.save()\n",
    "\n",
    "# === Load Agent ===\n",
    "# checkpoint = tf.train.Checkpoint(agent=agent, optimizer=agent._optimizer)\n",
    "# checkpoint.restore(tf.train.latest_checkpoint(agent_dir))\n",
    "\n",
    "# # save policy\n",
    "# tf_policy_saver = policy_saver.PolicySaver(agent.policy)\n",
    "# tf_policy_saver.save(agent_dir+\" Policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a32da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Look at the Q values alongside the data to see agent decisions \"\"\"\n",
    "\n",
    "\"\"\" Data Collection \"\"\"\n",
    "test_py_env.logger = []\n",
    "test_py_env.logging_step = test_py_env._L - 1\n",
    "q_values_over_time = []\n",
    "file_name = test_py_env._FILE_NAME\n",
    "\n",
    "time_step = test_py_env.reset()\n",
    "policy_state = agent.policy.get_initial_state(1)\n",
    "\n",
    "while not bool(time_step.is_last()):\n",
    "    test_py_env.logging_step += 1\n",
    "    \n",
    "    observation = tf.nest.map_structure(lambda x: np.expand_dims(x,axis= 0), time_step.observation)\n",
    "    step_type = tf.nest.map_structure(lambda x: np.expand_dims(x, axis = 0), time_step.step_type)\n",
    "    time_step = time_step._replace(observation = observation, step_type = step_type)\n",
    "    \n",
    "    # compute q_values pre action\n",
    "    q_values,_ = agent._q_network(\n",
    "        observation,  # add the batch dim\n",
    "        step_type,   # input T = 0\n",
    "        training=False\n",
    "    ) # Out [B, A]\n",
    "    q_values_over_time.append(tf.squeeze(q_values).numpy())\n",
    "    \n",
    "    # take the action\n",
    "    action_step = agent.policy.action(time_step, policy_state)\n",
    "    policy_state = action_step.state\n",
    "    \n",
    "    # update the time step\n",
    "    time_step = test_py_env.step(action_step.action)\n",
    "    \n",
    "df = pd.DataFrame.from_records(test_py_env.logger, index=\"step\")\n",
    "\n",
    "# --- Prepare df columns for plotting --\n",
    "\n",
    "# Prepopulate with holds\n",
    "df['plot_category'] = 'hold'\n",
    "\n",
    "# When action changes, show the *new* action\n",
    "change_mask = df['action'] != df['prev_action']\n",
    "df.loc[change_mask, 'plot_category'] = df.loc[change_mask, 'action']\n",
    "\n",
    "# mapping -> color & size for plotting\n",
    "plot_styles = {\n",
    "    'short': {'color': 'orange', 'size': 50},\n",
    "    'noTrade': {'color': 'purple', 'size': 50},\n",
    "    'long': {'color': 'green', 'size': 50},\n",
    "    'hold': {'color': 'cyan', 'size': 10},\n",
    "}\n",
    "\n",
    "# Total money made\n",
    "total_reward = df['eval_reward'].sum()\n",
    "\n",
    "# Convert q_values to numpy array and steps\n",
    "q_values_array = np.array(q_values_over_time)  # shape: [T, num_actions]\n",
    "steps = df.index.to_numpy()\n",
    "\n",
    "# --- Plotting ---\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Plot price with colored markers by category (only at relevant steps)\n",
    "for cat, style in plot_styles.items():\n",
    "    mask = df['plot_category'] == cat\n",
    "    ax1.scatter(df.index[mask], df['data'][mask],\n",
    "                color=style['color'], s=style['size'], label=cat)\n",
    "\n",
    "ax1.set_ylabel(\"Price\")\n",
    "ax1.set_title(f\"Agent Decisions on {file_name} | Money Made: {total_reward:.5f}\")\n",
    "ax1.legend()\n",
    "\n",
    "# --- Second plot: Q-values ---\n",
    "n_actions = q_values_array.shape[1]\n",
    "for i in range(n_actions):\n",
    "    color = list(plot_styles.values())[i]['color']  # same color order\n",
    "    ax2.plot(steps, q_values_array[:, i],\n",
    "                label=f\"Q({i})\", alpha=0.7, color=color)\n",
    "\n",
    "ax2.set_ylabel(\"Q-Values\")\n",
    "ax2.set_xlabel(\"Step\")\n",
    "ax2.set_title(\"Q-Values Over Time\")\n",
    "ax2.legend(loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599d0bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "# from collections import Counter\n",
    "\n",
    "# # params\n",
    "# K = 3\n",
    "# S = 1\n",
    "# dilations = (1,2,2,3)\n",
    "\n",
    "# # build the flat list of dilations, two layers per dilation per stack\n",
    "# layers = []\n",
    "# for _ in range(S):\n",
    "#     for d in dilations:\n",
    "#         layers.append(d)   # first conv in residual block\n",
    "#         layers.append(d)   # second conv in residual block\n",
    "\n",
    "# # for each layer, the kernel can “pick” any of the K positions {0, 1, …, K-1},\n",
    "# # which corresponds to an offset of (position * dilation)\n",
    "# offsets_per_layer = [\n",
    "#     [pos * d for pos in range(K)]\n",
    "#     for d in layers\n",
    "# ]\n",
    "\n",
    "# # compute all path‐delays (sum of one offset per layer)\n",
    "# all_delays = (\n",
    "#     sum(path)\n",
    "#     for path in itertools.product(*offsets_per_layer)\n",
    "# )\n",
    "\n",
    "# # tally them up\n",
    "# hist = Counter(all_delays)\n",
    "\n",
    "# # convert Counter to a plain dict\n",
    "# delay_to_paths = dict(hist)\n",
    "\n",
    "# pprint(delay_to_paths)\n",
    "\n",
    "# total_count = 0\n",
    "# max_path_len = max(list(delay_to_paths.keys()))\n",
    "# heuristic = int(0.8*max_path_len)\n",
    "# heuristic_count = 0\n",
    "\n",
    "\n",
    "# for p_len, count in delay_to_paths.items():\n",
    "#     total_count += count\n",
    "#     if p_len < heuristic:\n",
    "#         heuristic_count += count\n",
    "    \n",
    "# print(f\"Total count: {total_count}\")\n",
    "# print(f\"Heuristic count: {heuristic_count}\")\n",
    "# print(f\"Percent of all paths close by heuristic look back period of {heuristic}:  {heuristic_count/total_count*100}\")\n",
    "# print(f\"Max Path length {max_path_len}\")\n",
    "# print(f\"K:{K}, nb_stacks:{S}, dilations:{dilations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4bbfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "# from tensorflow.keras.optimizers.legacy import Adam\n",
    "# from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "# from tf_agents.networks.network import Network\n",
    "# # Assume CustomNetwork is already defined/imported in your environment\n",
    "\n",
    "# # --- 1. Generate fake data ---\n",
    "# # State dimension: Look-back L = 28, feature dim F = 8\n",
    "# # Action dimension A = 4\n",
    "# X = np.random.randn(1000, 22, 8).astype(np.float32)\n",
    "# y = np.random.randn(1000, 4).astype(np.float32)\n",
    "\n",
    "# # --- 2. Build the network ---\n",
    "# # Example hyperparameters\n",
    "# fc_units = [256, 128]       # will be ignored since we freeze heads only\n",
    "# regularization = 1e-4\n",
    "\n",
    "# network = CustomNetwork(\n",
    "#     observation_spec= train_py_env.observation_spec(),  # Replace with your actual spec if needed\n",
    "#     action_spec=train_py_env.action_spec(),       # Replace with your actual spec if needed\n",
    "#     fc_layer_params=fc_units,\n",
    "#     regularization=regularization\n",
    "# )\n",
    "\n",
    "# # Unfreeze all layers so gradients flow through both TCN and heads\n",
    "# for layer in network.layers:\n",
    "#     layer.trainable = True\n",
    "#     if layer.name in {\"Value_Head\", \"Advantage_Head\"}: layer.trainable =False\n",
    "\n",
    "# # --- 3. Prepare dataset ---\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((X, y)).batch(32)\n",
    "\n",
    "# # --- 4. Optimizer and loss ---\n",
    "# # Learning rate schedule (example)\n",
    "# init_lr = 1e-4\n",
    "# num_decay_steps = 1000\n",
    "# final_lr = 1e-5\n",
    "# lr_schedule = PolynomialDecay(init_lr, num_decay_steps, final_lr, power=1, cycle=False)\n",
    "# optimizer = Adam(learning_rate=lr_schedule)\n",
    "# mse = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# # --- 5. Training step function ---\n",
    "# @tf.function\n",
    "# def train_step(x_batch, y_batch):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         q_pred, _ = network(x_batch, training=True)  # [B, A]\n",
    "#         loss = mse(y_batch, q_pred)\n",
    "#     grads = tape.gradient(loss, network.trainable_variables)\n",
    "#     optimizer.apply_gradients(zip(grads, network.trainable_variables))\n",
    "#     return loss\n",
    "\n",
    "# # --- 6. Training loop ---\n",
    "# for epoch in range(100):\n",
    "#     epoch_loss = 0.0\n",
    "#     for xb, yb in dataset:\n",
    "#         batch_loss = train_step(xb, yb)\n",
    "#         epoch_loss += batch_loss\n",
    "#     print(f\"Epoch {epoch+1}, Loss: {epoch_loss.numpy()/len(dataset):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89637cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import per_buffer\n",
    "\n",
    "print(per_buffer.test())  # Should print: \"Hello from C++\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65323363",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
