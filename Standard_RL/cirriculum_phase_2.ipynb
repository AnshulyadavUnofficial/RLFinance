{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df658cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from enum import Enum\n",
    "import random \n",
    "\n",
    "\n",
    "num_times = 390\n",
    "start_dt = datetime(2000, 1, 1, 9, 30)  # arbitrary date\n",
    "time = [ (start_dt + timedelta(minutes=min)).time().strftime(\"%H:%M\") for min in range(num_times)]\n",
    "start_price = 100\n",
    "ema_span = 11\n",
    "to_plot = True\n",
    "\n",
    "# 1 because curr_open = next_close, so closes + first open,\n",
    "num_prices = num_times + ema_span + 1\n",
    "overall_day_change = 0.017 # a percent change over the entire day, if pure increase then a 15% increase\n",
    "\n",
    "t = np.arange(num_times)\n",
    "mid_index = num_times // 2 \n",
    "min_volume = 1000\n",
    "max_volume = 10000  # Peak volume at open/close\n",
    "power = 50\n",
    "volume_profile = min_volume + (max_volume - min_volume) * ((t - mid_index) / mid_index)**power\n",
    "\n",
    "def create_file_and_print(prices, volume, file_name = None, is_plot = True):\n",
    "    global time\n",
    "    \n",
    "    # common code for creating the data_frame and the graph\n",
    "    round_decimals = 4\n",
    "    open = np.round( prices[:-1], decimals= round_decimals )\n",
    "    close = np.round( prices[1:], decimals= round_decimals )\n",
    "    high = np.round(np.maximum(open,close), decimals = round_decimals )\n",
    "    low =np.round( np.minimum(open, close), decimals = round_decimals ) \n",
    "    \n",
    "    df = pd.DataFrame(data = {\n",
    "        \"Time\": time,\n",
    "        \"Close\": close,\n",
    "        \"High\": high,\n",
    "        \"Low\": low,\n",
    "        \"Open\": open,\n",
    "        \"Volume\": volume  # Add volume column\n",
    "    })\n",
    "    df[\"Time\"] = df[\"Time\"].astype(\"string\")\n",
    "\n",
    "    # Visualize\n",
    "    if is_plot:\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "        ax1.plot(range(num_times), close, label='Price')\n",
    "        ax1.set_title('Price vs Time')\n",
    "        ax2.bar(range(num_times), volume, color='gray', alpha=0.8)\n",
    "        ax2.set_title('U-Shaped Volume Profile')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    if file_name is not None: \n",
    "        df.to_csv(file_name, header=True, index=False)\n",
    "\n",
    "def generate_volume_with_ema(prices, baseline_volume, ema_span, k=0.4, C=0.1, sigma_v=0.4, threshold=0.0019, spike_mult=4):\n",
    "    \"\"\"\n",
    "    Generate a synthetic intraday volume series aligned to price behavior relative to an EMA.\n",
    "\n",
    "    Takes a full price history (including warm-up) and a baseline intraday volume profile of length T.\n",
    "    Computes an EMA over the entire price array, then for each of the T intervals:\n",
    "    - Measures the relative deviation of price from its EMA,\n",
    "    - Normalizes deviations and applies an exponential modulation to the baseline volume,\n",
    "    - Applies a spike multiplier when deviation exceeds a threshold,\n",
    "    - Adds log-normal noise.\n",
    "    Returns:\n",
    "    - volumes: array of length T with the synthetic volume for each interval,\n",
    "    - prices_out: the corresponding slice of T+1 price points used for OHLC alignment.\n",
    "    \"\"\"\n",
    "    prices = np.asarray(prices, dtype=float)\n",
    "    baseline = np.asarray(baseline_volume, dtype=float)\n",
    "    T = len(baseline)\n",
    "    warm_up = len(prices) - T -1\n",
    "    if warm_up < ema_span:\n",
    "        raise ValueError(\"Need at least ema_span warm-up points before baseline period\")\n",
    "\n",
    "    # Compute EMA reference series\n",
    "    ema = np.empty(len(prices), dtype=float)\n",
    "    alpha = 2.0 / (ema_span + 1)\n",
    "    ema[0] = prices[0]\n",
    "    for i in range(1, len(prices)):\n",
    "        ema[i] = alpha * prices[i] + (1 - alpha) * ema[i-1]\n",
    "\n",
    "    # Compute relative deviations and global RMS scale\n",
    "    prices_out = prices[warm_up: warm_up + T + 1]\n",
    "    rel_dev = np.abs(prices_out[:-1]/ ema[warm_up+1 :warm_up + T+1] - 1)  # array length T\n",
    "    sd = np.sqrt(np.mean(rel_dev**2))\n",
    "    \n",
    "    C_add = baseline.mean()*C* np.sqrt(1/np.pi)\n",
    "\n",
    "    volumes = np.zeros(T, dtype=int)\n",
    "    for i in range(T):\n",
    "    \n",
    "        # Modulation centered at 1\n",
    "        M = np.exp(k*(rel_dev[i] / sd - 1)) # try to keep the argument of the exponent around 0, since e^0 ~ 1\n",
    "        # Spike detection: threshold_mult\n",
    "        spike = 1.0\n",
    "        if rel_dev[i] > threshold :spike*= spike_mult\n",
    "        # Noise component\n",
    "        eps_v = sigma_v* np.random.standard_normal()\n",
    "        vol = baseline[i] * M * spike * np.exp(eps_v)\n",
    "        # The additive component\n",
    "        vol += C_add * max(0, 1 - rel_dev[i]/sd)\n",
    "        # Ensure at least 1\n",
    "        volumes[i] = max(1, int(round(vol)))\n",
    "\n",
    "    return volumes, prices_out\n",
    "\n",
    "\"\"\"\n",
    "Common function for breakout for consolidation, OU and jump diffusionxs\n",
    "\"\"\"\n",
    "\n",
    "def simulate_ou_process(start_price: float,duration: int,theta: float | np.ndarray = 0.0005,sigma: float | np.ndarray = 0.0002,mu: None | float | np.ndarray = None):\n",
    "    \"\"\"\n",
    "    Simulate an Ornstein-Uhlenbeck (mean-reverting) price process.\n",
    "\n",
    "    Args:\n",
    "        start_price (float): Initial price at time t = 0.\n",
    "        duration (int): Number of time steps to simulate.\n",
    "        theta (float or np.ndarray): Mean-reversion speed. If array, must be length `duration`.\n",
    "        sigma (float or np.ndarray): Volatility (noise scale). If array, must be length `duration`.\n",
    "        mu (None, float, or np.ndarray): Drift or long-run mean target.\n",
    "            - None: no drift; mean reversion to constant start_price.\n",
    "            - float: linear drift; mean_series = start_price * (1 + (i * mu) / duration).\n",
    "            - np.ndarray: custom mean_series; must have length `duration`.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Simulated price series of length `duration`.\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if duration <= 0:\n",
    "        raise ValueError(\"`duration` must be a positive integer\")\n",
    "\n",
    "    # Initialize price array\n",
    "    prices = np.zeros(duration, dtype=float)\n",
    "    prices[0] = start_price\n",
    "\n",
    "    # Construct target mean series\n",
    "    if mu is None:\n",
    "        mean_series = np.full(duration, start_price, dtype=float)\n",
    "    elif isinstance(mu, (int, float)):\n",
    "        if mu == 0:\n",
    "            mean_series = np.full(duration, start_price, dtype=float)\n",
    "        else:\n",
    "            # Linear drift towards a changing mean\n",
    "            mean_series = start_price * (1 + np.arange(duration) * (mu / duration))\n",
    "    elif isinstance(mu, np.ndarray):\n",
    "        if mu.shape[0] != duration:\n",
    "            raise ValueError(\"`mu` array length must equal `duration`\")\n",
    "        mean_series = mu.astype(float)\n",
    "    else:\n",
    "        raise ValueError(\"`mu` must be None, a float, or a numpy array of length `duration`\")\n",
    "\n",
    "    # Construct theta series\n",
    "    if isinstance(theta, (int, float)):\n",
    "        theta_series = np.full(duration, float(theta), dtype=float)\n",
    "    elif isinstance(theta, np.ndarray):\n",
    "        if theta.shape[0] != duration:\n",
    "            raise ValueError(\"`theta` array length must equal `duration`\")\n",
    "        theta_series = theta.astype(float)\n",
    "    else:\n",
    "        raise ValueError(\"`theta` must be a float or a numpy array of length `duration`\")\n",
    "\n",
    "    # Construct sigma series\n",
    "    if isinstance(sigma, (int, float)):\n",
    "        sigma_series = np.full(duration, float(sigma), dtype=float)\n",
    "    elif isinstance(sigma, np.ndarray):\n",
    "        if sigma.shape[0] != duration:\n",
    "            raise ValueError(\"`sigma` array length must equal `duration`\")\n",
    "        sigma_series = sigma.astype(float)\n",
    "    else:\n",
    "        raise ValueError(\"`sigma` must be a float or a numpy array of length `duration`\")\n",
    "\n",
    "    # Simulate OU process\n",
    "    for t in range(1, duration):\n",
    "        drift = theta_series[t] * (mean_series[t] - prices[t - 1])\n",
    "        noise = sigma_series[t] * np.random.standard_normal()\n",
    "        prices[t] = prices[t - 1] + drift + noise\n",
    "\n",
    "    return prices\n",
    "\n",
    "\"\"\"\n",
    "A baseline hold profile\n",
    "\"\"\"\n",
    "prices = np.full(391, start_price)\n",
    "create_file_and_print(prices, volume_profile.round(), \"../Cirricula/Phase 2/hold\", is_plot= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f61175",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple OU processes\n",
    "\"\"\"\n",
    "np.random.seed(100)\n",
    "\n",
    "# theta bounds\n",
    "theta_min = 0.02\n",
    "theta_max = 0.09\n",
    "# sigma bounds\n",
    "sigma_min = 0.11\n",
    "sigma_max = 0.21\n",
    "# mu bounds\n",
    "mu_min = -0.03\n",
    "mu_max = 0.03\n",
    "\n",
    "num_profiles = 100\n",
    "for file_ind in range(num_profiles):\n",
    "    sigma = np.random.uniform(sigma_min, sigma_max)\n",
    "    theta = np.random.uniform(theta_min, theta_max)\n",
    "    mu = np.random.uniform(mu_min, mu_max)\n",
    "    \n",
    "    prices = simulate_ou_process(start_price, num_prices, theta, sigma, mu)\n",
    "    volumes,prices = generate_volume_with_ema(prices, volume_profile, ema_span=ema_span)\n",
    "    file_name = \"../Cirricula/Phase 2/ou_\"\n",
    "    \n",
    "    if mu >= 0.02:file_name+=\"inc_fast_mu_\"\n",
    "    elif mu >=0.005:file_name+=\"inc_slow_mu_\"\n",
    "    elif -0.005< mu <0.005:file_name+=\"stationary_mu_\"\n",
    "    elif mu<= -0.005: file_name+=\"dec_slow_mu_\"\n",
    "    elif mu<= -0.02: file_name+=\"dec_fast_mu_\"\n",
    "    \n",
    "    file_name += f\"{file_ind}\"\n",
    "    create_file_and_print(prices, volumes, file_name, is_plot=False)\n",
    "    \n",
    "    if file_ind%10==0:\n",
    "        print(f\"avg dlog price: {1000*np.mean(np.abs(np.diff(np.log(prices))))}\")\n",
    "        print(f\"avg dlog volume: {np.mean(np.abs(np.log(volumes/volume_profile)))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd1b6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "\"\"\"\n",
    "Jump diffusion processess\n",
    "\"\"\"\n",
    "# the mu and sigma numbers are chosen so that there are 1-3 jumps in each profile \n",
    "mu = 0.78\n",
    "sigma = 0.25\n",
    "num_trends = 100\n",
    "trend_lengths = (np.random.normal(mu, sigma, num_trends)*num_prices).astype(int)\n",
    "trend_lengths.sort()\n",
    "# theta bounds\n",
    "theta_min = 0.01\n",
    "theta_max = 0.09\n",
    "# sigma bounds\n",
    "sigma_min = 0.1\n",
    "sigma_max = 0.2\n",
    "# mu bounds\n",
    "mu_min = -0.005\n",
    "mu_max = 0.005\n",
    "\n",
    "min_jump_pcnt = 0.01\n",
    "max_jump_pcnt = 0.03\n",
    "\n",
    "num_profiles = 100\n",
    "for file_ind in range(num_profiles):\n",
    "    trend_lens = []\n",
    "    while sum(trend_lens) < num_prices:\n",
    "        trend_lens.append(np.random.choice(trend_lengths))\n",
    "    trend_lens[-1] = num_prices - sum(trend_lens[:-1])\n",
    "    \n",
    "    prices = np.array([])\n",
    "    for trend_len in trend_lens:\n",
    "        sigma = np.random.uniform(sigma_min, sigma_max)\n",
    "        theta = np.random.uniform(theta_min, theta_max)\n",
    "        mu = np.random.uniform(mu_min, mu_max)\n",
    "        curr_prices = simulate_ou_process(start_price, trend_len, theta, sigma, mu)\n",
    "        \n",
    "        # add a jump between segments\n",
    "        if prices.size != 0: # ie has a previous segment\n",
    "            jump_percent = np.random.uniform(min_jump_pcnt, max_jump_pcnt)\n",
    "            jump = start_price * jump_percent * random.choice([-1, 1]) # a positive or negative jump\n",
    "            offset = (prices[-1] + jump) - curr_prices[0]\n",
    "            curr_prices += offset\n",
    "        prices = np.concatenate([prices, curr_prices], axis=0)\n",
    "\n",
    "    volumes,prices = generate_volume_with_ema(prices, volume_profile, ema_span=ema_span)\n",
    "    \n",
    "    file_name = f\"../Cirricula/Phase 2/jd_{len(trend_lens)}_jump_{file_ind}\"\n",
    "    create_file_and_print(prices, volumes, file_name, is_plot=False)\n",
    "    if file_ind%10==0:\n",
    "        print(f\"avg dlog price: {1000*np.mean(np.abs(np.diff(np.log(prices))))}\")\n",
    "        print(f\"avg dlog volume: {np.mean(np.abs(np.log(volumes/volume_profile)))}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a06af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(999)\n",
    "\"\"\" \n",
    "breakout from consolidation\n",
    "\"\"\"\n",
    "# creating trend lines using desmos values for mu=0.5 and sigma so that no values are negative and always close to mu.\n",
    "mu=0.5\n",
    "sigma=0.10\n",
    "num_trends = 20\n",
    "trend_lines = (np.random.normal(mu,sigma, num_trends)*num_prices).astype(int)\n",
    "trend_lines.sort()\n",
    "\n",
    "# parameters for stationary phase\n",
    "theta_stat_start = 0.001\n",
    "theta_stat_end = 0.04       # A larger theta means faster return to mean\n",
    "\n",
    "mean_stat_min = 0\n",
    "mean_stat_max = 0.005\n",
    "\n",
    "# parameters for trend phase\n",
    "theta_trend_min = 0.01\n",
    "theta_trend_max = 0.09       \n",
    "\n",
    "mean_trend_min = 0.01\n",
    "mean_trend_max = 0.05\n",
    "\n",
    "# common parameters\n",
    "sigma_min = 0.1\n",
    "sigma_max = 0.2\n",
    "\n",
    "num_profiles = 100\n",
    "for file_ind in range(num_profiles):\n",
    "    # Creating the stationary phase\n",
    "    stat_dur = np.random.choice(trend_lines)\n",
    "\n",
    "    stat_theta = np.linspace(theta_stat_start, theta_stat_end, stat_dur)\n",
    "    stat_sigma = np.random.uniform(sigma_min, sigma_max)\n",
    "    stat_mu = np.random.uniform(mean_stat_min, mean_stat_max)*np.random.choice([-1,1])\n",
    "    \n",
    "    stat_prices =  simulate_ou_process(start_price, stat_dur, stat_theta, stat_sigma, stat_mu)\n",
    "    \n",
    "    # creating the trending phase\n",
    "    trend_dur = num_prices - stat_dur\n",
    "    \n",
    "    trend_theta = np.random.uniform(theta_trend_min, theta_trend_max)\n",
    "    trend_sigma = np.random.uniform(sigma_min, sigma_max)\n",
    "    trend_mu = np.random.uniform(mean_trend_min, mean_trend_max)*np.random.choice([-1,1])\n",
    "    \n",
    "    trend_prices =  simulate_ou_process(start_price, trend_dur, trend_theta, trend_sigma, trend_mu)\n",
    "    \n",
    "    # combining phases\n",
    "    offset = stat_prices[-1] - trend_prices[0]\n",
    "    trend_prices +=offset\n",
    "    prices = np.concatenate([stat_prices, trend_prices], axis=0)\n",
    "    \n",
    "    volumes,prices = generate_volume_with_ema(prices, volume_profile, ema_span=ema_span)\n",
    "    \n",
    "    # creating discernable file_name \n",
    "    file_name = f\"../Cirricula/Phase 2/bfc_for_\" \n",
    "    \n",
    "    if abs(trend_mu) < (mean_trend_max + mean_trend_min)/2: file_name+=\"small_\"\n",
    "    else: file_name+=\"large_\"\n",
    "    \n",
    "    if trend_mu >0: file_name+=\"pos_mean_\"\n",
    "    else: file_name+=\"neg_mean_\"\n",
    "    \n",
    "    if trend_dur < num_prices/2: file_name += \"short_trend\"\n",
    "    else: file_name += \"long_trend\"\n",
    "    \n",
    "    file_name += f\"_{file_ind}\"\n",
    "    \n",
    "    create_file_and_print(prices, volumes, file_name, is_plot=False)\n",
    "    \n",
    "    if file_ind%10==0:\n",
    "        print(f\"avg dlog price: {1000*np.mean(np.abs(np.diff(np.log(prices))))}\")\n",
    "        print(f\"avg dlog volume: {np.mean(np.abs(np.log(volumes/volume_profile)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bcd38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Momentum with Volume Acceleration & Taper\n",
    "\"\"\"\n",
    "np.random.seed(100)\n",
    "def generate_sigmoid_mean_series( total_pct_inc, horizontal_stretch_pct=0.05, center_index=None):\n",
    "    \"\"\"\n",
    "    Generate an S-shaped mean series using a sigmoid function.\n",
    "\n",
    "    Parameters:\n",
    "    - total_pct_inc: float, total percentage increase over the series (e.g., 0.01 for +1%).\n",
    "    - horizontal_stretch_pct: float, controls steepness; larger values make transition sharper.\n",
    "    - center_index: int or None, index at which midpoint of sigmoid occurs; if None, defaults to length//2.\n",
    "\n",
    "    Returns:\n",
    "    - mean_series: np.ndarray of shape (num_prices,), values starting near start_price and\n",
    "      rising toward start_price*(1 + total_pct_inc).\n",
    "    \"\"\"\n",
    "    global start_price, num_prices\n",
    "    times = np.arange(num_prices)\n",
    "    if center_index is None:\n",
    "        center_index = num_prices // 2\n",
    "\n",
    "    # Compute input for sigmoid: shift times so midpoint at center_index\n",
    "    x = (times - center_index) * horizontal_stretch_pct\n",
    "\n",
    "    # Sigmoid function: stable form 1 / (1 + exp(-x))\n",
    "    sig = 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Scale to start_price → start_price*(1 + total_pct_inc)\n",
    "    mean_series = start_price * (1 + total_pct_inc * sig)\n",
    "    return mean_series\n",
    "\n",
    "theta = 0.10\n",
    "sigma = 0.1\n",
    "min_pcent_inc = 0.009\n",
    "max_pcent_inc = 0.031\n",
    "min_h_stretch = 0.03\n",
    "max_h_stretch = 0.12\n",
    "min_center = int(0.2*num_prices)\n",
    "max_center = int(0.8*num_prices)\n",
    "\n",
    "num_profiles = 100\n",
    "for file_ind in range(num_profiles):\n",
    "    \n",
    "    h = np.random.uniform(min_h_stretch, max_h_stretch)\n",
    "    \n",
    "    total_pcent_inc = np.random.uniform(min_pcent_inc, max_pcent_inc)\n",
    "    total_pcent_inc*=np.random.choice([-1,1])\n",
    "    \n",
    "    center = np.random.randint(min_center, max_center)\n",
    "    mu = generate_sigmoid_mean_series(total_pcent_inc, h, center)\n",
    "\n",
    "    prices = simulate_ou_process(start_price, duration = num_prices,theta=theta,sigma=sigma, mu= mu)\n",
    "\n",
    "    volumes,prices = generate_volume_with_ema(prices, volume_profile, ema_span=ema_span)\n",
    "    \n",
    "    file_name = f\"../Cirricula/Phase 2/acc_then_taper_\"\n",
    "    if total_pcent_inc >0:\n",
    "        if total_pcent_inc >(max_pcent_inc + min_pcent_inc)/2:\n",
    "            file_name +=\"pos_large\"\n",
    "        else: file_name +=\"pos_small\"\n",
    "    else:\n",
    "        if -total_pcent_inc > (max_pcent_inc + min_pcent_inc)/2:\n",
    "            file_name +=\"neg_large\"\n",
    "        else: file_name += \"neg_small\"\n",
    "    file_name += f\"_{file_ind}\"\n",
    "    \n",
    "    create_file_and_print(prices, volumes, file_name, is_plot=False)\n",
    "    plt.plot(prices)\n",
    "    if file_ind%10==0:\n",
    "        print(f\"avg dlog price: {1000*np.mean(np.abs(np.diff(np.log(prices))))}\")\n",
    "        print(f\"avg dlog volume: {np.mean(np.abs(np.log(volumes/volume_profile)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacb6fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Regime-Switching Trend and Volatility\n",
    "\"\"\"\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "# mu and sigma are chosen using a visual inspection from desmos, so that each profile contains 3-5 different segment\n",
    "num_trend_samples = 100\n",
    "mu = 0.28\n",
    "sigma = 0.10\n",
    "trend_lengths = (np.random.normal(mu, sigma, num_trend_samples)*num_prices).astype(int)\n",
    "trend_lengths.sort()\n",
    "\n",
    "means_dict = {\"pos_small\":0.01, \"pos_mid\":0.02, \"pos_large\":0.03, \"no_trend\":0.00, \"neg_small\":-0.01, \"neg_mid\":-0.02, \"neg_large\":-0.03}\n",
    "means_dict_keys = list(means_dict.keys())\n",
    "sigma_dict = {\"small\":0.1, \"mid\":0.12, \"large\":0.15}\n",
    "sigma_dict_keys = list(sigma_dict.keys())\n",
    "theta = 0.1\n",
    "\n",
    "num_profiles = 100\n",
    "for file_ind in range(num_profiles):\n",
    "    trend_lens = []\n",
    "    while sum(trend_lens) < num_prices:\n",
    "        trend_lens.append(random.choice(trend_lengths))\n",
    "    # remove the last element since it overshot num_prices and replace it with the revelant length\n",
    "    trend_lens[-1] = num_prices - sum(trend_lens[:-1])\n",
    "    file_name = \"../Cirricula/Phase 2/rst_\"\n",
    "    prices = np.array([])\n",
    "    \n",
    "    for trend_len in trend_lens:\n",
    "        # trend type\n",
    "        trend_type = random.choice(means_dict_keys)\n",
    "        # assign trend mean\n",
    "        trend_mean = means_dict[trend_type]\n",
    "        # assign trend sigma\n",
    "        if trend_type == \"no_trend\": trend_sigma = sigma_dict[\"small\"]\n",
    "        else: trend_sigma = sigma_dict[trend_type.split(\"_\")[-1]]\n",
    "        # get curr prices\n",
    "        curr_prices = simulate_ou_process(start_price,duration=trend_len,theta=theta,sigma=trend_sigma,mu=trend_mean)\n",
    "        \n",
    "        # add a small jump between segments\n",
    "        if prices.size != 0: # ie has a previous segment\n",
    "            jump_percent = trend_mean*0.1\n",
    "            jump = start_price * jump_percent * random.choice([-1, 1])  # ±0.5%\n",
    "            offset = (prices[-1] + jump) - curr_prices[0]\n",
    "            curr_prices += offset\n",
    "        \n",
    "        file_name+= f\"{trend_type}_\"\n",
    "        prices = np.concatenate([prices, curr_prices], axis=0)\n",
    "\n",
    "    volumes,prices = generate_volume_with_ema(prices, volume_profile, ema_span=ema_span)\n",
    "    file_name = file_name[:-1] # remove the last underscore\n",
    "    file_name = f\"{file_name}_{file_ind}\"\n",
    "    create_file_and_print(prices, volumes, file_name, is_plot=False)\n",
    "    if file_ind%10==0:\n",
    "        print(f\"avg dlog price: {1000*np.mean(np.abs(np.diff(np.log(prices))))}\")\n",
    "        print(f\"avg dlog volume: {np.mean(np.abs(np.log(volumes/volume_profile)))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814fbb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Volatility Clustering Segments\n",
    "\"\"\"\n",
    "\n",
    "class Regime(Enum):\n",
    "    low = -1\n",
    "    high = 1\n",
    "\n",
    "    def regime_change(self):\n",
    "        if self == self.low: return self.high\n",
    "        if self == self.high: return self.low\n",
    "        raise ValueError(f\"current regime is not a proper regime.\")\n",
    "\n",
    "def simulate_volatility_clustering(start_regime:Regime, low_vol:float, high_vol:float, switch_prob:float, h_reset:bool, h_alpha:float):\n",
    "    global start_price, num_prices\n",
    "    curr_regime = start_regime\n",
    "    \n",
    "    prices = [start_price]\n",
    "    h_t = 1\n",
    "    for t in range(1,num_prices):\n",
    "        # Regime switch decision\n",
    "        if np.random.standard_normal() < switch_prob:\n",
    "            curr_regime = curr_regime.regime_change()\n",
    "            if h_reset: h_t = 1\n",
    "        \n",
    "        # Base volatility from regime\n",
    "        base_vol = low_vol if curr_regime == Regime.low else high_vol\n",
    "        \n",
    "        # Apply persistence factor: actual volatility = base_vol * sqrt(h_t)\n",
    "        vol_t = base_vol * np.sqrt(h_t)\n",
    "        r = vol_t * np.random.standard_normal()\n",
    "        new_price = prices[-1] * np.exp(r)\n",
    "        prices.append(new_price)\n",
    "        \n",
    "        # Update persistence factor h_t using EMA of squared normalized return\n",
    "        # normalized by base_vol: (r / base_vol)^2\n",
    "        h_t = h_alpha * h_t + (1 - h_alpha) * (r/base_vol)**2\n",
    "    return np.array(prices)\n",
    "\n",
    "seeds = [1000 + i for i in range(10)]\n",
    "for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "    # 1. Basic profile. \n",
    "    # Effect: start in low regime, with not too low low_vol and not too high high_vol, decent switch_prob and with resets at each regime change.\n",
    "    start_regime = Regime.low\n",
    "    low_vol = 0.001 # volatility\n",
    "    high_vol = 0.002 # volatility\n",
    "    switch_prob = 0.04 \n",
    "    h_reset=True\n",
    "    h_alpha = 0.9 # the alpha for ema calculation\n",
    "\n",
    "    prices = simulate_volatility_clustering(start_regime,low_vol,high_vol,switch_prob,h_reset,h_alpha) \n",
    "    volumes,prices = generate_volume_with_ema(prices, volume_profile, ema_span=ema_span)\n",
    "    file_name = f\"../Cirricula/Phase 2/vc_basic_{seed}\"\n",
    "    create_file_and_print(prices, volumes, file_name, is_plot=False)\n",
    "\n",
    "    # 2.Long Quiet, Sharp Spikes\n",
    "    #Effect: Long calm periods; occasional regime switch to very noisy episodes, but persistence within each.\n",
    "    start_regime = Regime.low\n",
    "    low_vol=0.001\n",
    "    high_vol=0.005 # (high-vol much larger than low)\n",
    "    switch_prob=0.02 # (rare switches)\n",
    "    h_alpha=0.95 #(high persistence once a shock occurs)\n",
    "    h_reset=True #(reset persistence on switch, so high-vol only from new shocks)\n",
    "\n",
    "    prices = simulate_volatility_clustering(start_regime,low_vol,high_vol,switch_prob,h_reset,h_alpha) \n",
    "    volumes,prices = generate_volume_with_ema(prices, volume_profile, ema_span=ema_span)\n",
    "    file_name = f\"../Cirricula/Phase 2/vc_long_quiet_sharp_spike_{seed}\"\n",
    "    create_file_and_print(prices, volumes, file_name, is_plot=False)\n",
    "\n",
    "    # 3. Moderate Clustering\n",
    "    # Effect: Balanced quiet/active durations; moderate persistence.\n",
    "    start_regime = Regime.low\n",
    "    low_vol=0.001\n",
    "    high_vol=0.002\n",
    "    switch_prob=0.05 #(moderate frequency)\n",
    "    h_alpha=0.9\n",
    "    h_reset=True\n",
    "\n",
    "    prices = simulate_volatility_clustering(start_regime,low_vol,high_vol,switch_prob,h_reset,h_alpha) \n",
    "    volumes,prices = generate_volume_with_ema(prices, volume_profile, ema_span=ema_span)\n",
    "    file_name = f\"../Cirricula/Phase 2/vc_moderate_clustering_{seed}\"\n",
    "    create_file_and_print(prices, volumes, file_name, is_plot=False)\n",
    "\n",
    "    # 4.Frequent Switching, Low Persistence\n",
    "    #Effect: Many short bursts of noise, quickly reverting; tests agent’s adaptability.\n",
    "    start_regime = Regime.low\n",
    "    low_vol=0.002\n",
    "    high_vol=0.005\n",
    "    switch_prob=0.1 #(frequent switches)\n",
    "    h_alpha=0.7 #(fast decay of volatility after shocks)\n",
    "    h_reset=False #(with low persistence reset less critical)\n",
    "\n",
    "    prices = simulate_volatility_clustering(start_regime,low_vol,high_vol,switch_prob,h_reset,h_alpha) \n",
    "    volumes,prices = generate_volume_with_ema(prices, volume_profile, ema_span=ema_span)\n",
    "    file_name = f\"../Cirricula/Phase 2/vc_frequent_switching_{seed}\"\n",
    "    create_file_and_print(prices, volumes, file_name, is_plot=False)\n",
    "\n",
    "    # 5. High Persistence Across Switches\n",
    "    # Effect: After a big shock, volatility remains elevated even if regime flips back to “low”; long tails of high volatility.\n",
    "    start_regime = Regime.low\n",
    "    low_vol=0.001\n",
    "    high_vol=0.004\n",
    "    switch_prob=0.03\n",
    "    h_alpha=0.98 #(very slow decay)\n",
    "    h_reset=False #(carry over shocks into next regime)\n",
    "\n",
    "    prices = simulate_volatility_clustering(start_regime,low_vol,high_vol,switch_prob,h_reset,h_alpha) \n",
    "    volumes,prices = generate_volume_with_ema(prices, volume_profile, ema_span=ema_span)\n",
    "    file_name = f\"../Cirricula/Phase 2/vc_high_persistence_switching_{seed}\"\n",
    "    create_file_and_print(prices, volumes, file_name, is_plot=False)\n",
    "\n",
    "    # 6. Quick Reversion After Shock\n",
    "    # Effect: Shocks spike volatility, but it decays rapidly even if regime remains active; mixed realism.\n",
    "    start_regime = Regime.low\n",
    "    low_vol=0.001\n",
    "    high_vol=0.003\n",
    "    switch_prob=0.05\n",
    "    h_alpha=0.6 #(fast decay)\n",
    "    h_reset=True\n",
    "\n",
    "    prices = simulate_volatility_clustering(start_regime,low_vol,high_vol,switch_prob,h_reset,h_alpha) \n",
    "    volumes,prices = generate_volume_with_ema(prices, volume_profile, ema_span=ema_span)\n",
    "    file_name = f\"../Cirricula/Phase 2/vc_quick_reversion_after_shock_{seed}\"\n",
    "    create_file_and_print(prices, volumes, file_name, is_plot=False)\n",
    "\n",
    "    # 7. Asymmetric Volatility Levels\n",
    "    # Effect: Very calm baseline; when active, noise is large. Agent sees stark contrast.\n",
    "    start_regime = Regime.low\n",
    "    low_vol=0.0002\n",
    "    high_vol=0.004\n",
    "    switch_prob=0.04\n",
    "    h_alpha=0.9\n",
    "    h_reset=True\n",
    "\n",
    "    prices = simulate_volatility_clustering(start_regime,low_vol,high_vol,switch_prob,h_reset,h_alpha) \n",
    "    volumes,prices = generate_volume_with_ema(prices, volume_profile, ema_span=ema_span)\n",
    "    file_name = f\"../Cirricula/Phase 2/vc_asymmetric_volatility_{seed}\"\n",
    "    create_file_and_print(prices, volumes, file_name, is_plot=False)\n",
    "\n",
    "    # 8. Basic high vol profile. \n",
    "    # Effect: start in low regime, with not too low low_vol and not too high high_vol, decent switch_prob and with resets at each regime change.\n",
    "    start_regime = Regime.high\n",
    "    low_vol = 0.001 # volatility\n",
    "    high_vol = 0.002 # volatility\n",
    "    switch_prob = 0.04 \n",
    "    h_reset=True\n",
    "    h_alpha = 0.9 # the alpha for ema calculation\n",
    "\n",
    "    prices = simulate_volatility_clustering(start_regime,low_vol,high_vol,switch_prob,h_reset,h_alpha) \n",
    "    volumes,prices = generate_volume_with_ema(prices, volume_profile, ema_span=ema_span)\n",
    "    file_name = f\"../Cirricula/Phase 2/vc_high_basic_{seed}\"\n",
    "    create_file_and_print(prices, volumes, file_name, is_plot=False)\n",
    "\n",
    "    print(f\"avg dlog price: {1000*np.mean(np.abs(np.diff(np.log(prices))))}\")\n",
    "    print(f\"avg dlog volume: {np.mean(np.abs(np.log(volumes/volume_profile)))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121dfb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create the date_file.txt\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path(r\"../Cirricula/Phase 2\")\n",
    "raw_files: set[Path] = set()\n",
    "\n",
    "for file in root.rglob(\"*\"):\n",
    "    if file.name == \"date_file.txt\" or file.name == \"hold\": continue\n",
    "    raw_files.add(file)\n",
    "    \n",
    "# replace create date_file and write to it\n",
    "with open(r\"../Cirricula/Phase 2/date_file.txt\", \"w\") as date_file:\n",
    "    # write hold twice\n",
    "    date_file.write(\"hold\\n\")\n",
    "    date_file.write(\"hold\\n\")\n",
    "    \n",
    "    # for each file in rawfile write hold and then the file name\n",
    "    for file in raw_files:\n",
    "        date_file.write(\"hold\\n\")\n",
    "        date_file.write(file.name+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "280797ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1 Mean: 0.94291, Std: 0.72652\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Check dlog values for actual data volume and prices\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "root = Path(r\"../US_Market_Data\")\n",
    "# root = Path(r\"../Cirricula/Phase 2\")\n",
    "root = Path(r\"../Cirricula/Phase 1\")\n",
    "count=0\n",
    "average_price_movements = []\n",
    "reward_scalar = 1500\n",
    "\n",
    "\n",
    "for file in root.rglob(\"*\"):\n",
    "    if file.is_dir() or file.name == \"date_file.txt\":continue\n",
    "    df = pd.read_csv(file, index_col=0, header=0)\n",
    "    \n",
    "    open = df[\"Open\"].to_numpy()\n",
    "    close = df[\"Close\"].to_numpy()\n",
    "    high = df[\"High\"].to_numpy()\n",
    "    low = df[\"Low\"].to_numpy()\n",
    "    volume = df[\"Volume\"].to_numpy()\n",
    "    average_price_movements.append(  np.mean(np.abs( np.log(close/open) )) )\n",
    "    # if count%10==0:\n",
    "    #     print(f\"avg dlog price: {1000*np.mean(np.abs( np.log(close/open) ))}\")\n",
    "    #     print(f\"avg dlog volume: {np.mean(np.abs(np.log(volume/volume_profile)))}\")\n",
    "    count+=1    \n",
    "    \n",
    "average_price_movements = np.array(average_price_movements) * reward_scalar\n",
    "\n",
    "mean = np.mean(average_price_movements)\n",
    "std = np.std(average_price_movements)\n",
    "print(f\"{root.name} Mean: {mean:.5f}, Std: {std:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861c421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.log(close/open).round(8)\n",
    "cumul_data = np.cumsum(data)\n",
    "\n",
    "print(cumul_data)\n",
    "plt.plot(cumul_data)\n",
    "plt.show()\n",
    "plt.plot(close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900c90d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf2d75e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
